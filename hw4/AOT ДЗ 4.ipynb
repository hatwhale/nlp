{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Составить (с использованием любого модуля морфологического анализа) программу, выполняющую синтаксическую сегментацию русскоязычного текста на базе нескольких выбранных правил:\n",
    "* сегментацию на простые предложения по знакам пунктуации \t\tи/или\n",
    "* выделение неразрывных синтаксически связанных групп слов на основе локальных высоковероятных связей (примеры правил есть на слайдах 34, 50, 51 презентации по СА), например: синий платок, очень красивый (или даже более сложные группы: исключительно интересный фильм, красный полосатый мяч, любит весело играть, которые выделяются применением нескольких правил).\n",
    "\n",
    "Протестировать программу на нескольких фрагментах текста (не менее 1-2 страниц).\n",
    "\n",
    "Отчет: описание применяемых правил сегментации, результаты тестов, программа и комментарий к ней."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Предисловие"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сразу отметим, что в наши цели не входит написание идеальной сегментации (которая работает для любых текстов и учитывает различные \"исключительные\" случаи), идеального синтаксического анализатора (который работает для любых предложений и строит правильные синтаксические деревья) и так далее. В первую очередь, это связано с самим заданием, предполагающим следование некоторым правилам, как-то:\n",
    "* сегментация по знакам пунктуации -- простейшая сегментация не учитывает сокращения вида \"А. С. Пушкин\", некорректно обратывает предложение с прямой речью в нем и проч.;\n",
    "* выделение синтаксически связанных групп слов на основе локальных высоковероятных связей -- набор правил, данный в презентациях (и может даже несколько расширенный) не дает возможности построить корректное дерево зависимостей для произвольного предложения (помимо того, что дерево зависимостей обладает своими недостатками, применение этих правил также не дает гарантированно верного результата).\n",
    "\n",
    "Поэтому обговорим задачи, которые были поставлены и выполнены в этом задании:\n",
    "* сколько-нибудь качественная сегментация литературных текстов, следующая принципам \"наивной\" графематики с некототорыми уточнениями, связанными с форматом текстов, нами взятых;\n",
    "* реализация правил, данных в презентации, с некоторыми уточнениями, связанными с работой используемого морфоанализатора;\n",
    "* выделение синтаксически связанных групп слов путем синтаксического анализа каждого предложения, выполняющегося путем проверки наличия связи каждого слова с его \"соседями\" (с уточнением того, что стоит понимать под \"соседними\" словами);\n",
    "* и, наконец, описание недостатков построенных моделей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Часть 1. Сегментация текста на предложения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Нами будут рассматриваться тексты, взятые с сайта lib.ru: они были банально скопированы в текстовый файл во избежание работы с html-файлами. В рассматриваемых текстах есть членение текста на абзацы, которое длостаточно просто выделяется, также в предложениях присутствуют символы '\\n', которые реализуют перенос строк.\n",
    "\n",
    "Создадим класс <b>Split</b>, который реализует сегментацию полученных текстов (заданных по имени файлов) на:\n",
    "* абзацы - путем выделения \"красных\" строк;\n",
    "* предложения - путем разбиения по терминальным знакам пунктуации (.!? в различных их комбинациях);\n",
    "* \"подпредложения\" - путем разбиения по знакам пунктуации (.!?:,), в дальнейшем использоваться не будет.\n",
    "\n",
    "Стоит отметить, что написанные типы сегментации могут применяться независимо друг от друга, это влияет лишь на полученную структуру текста: при каждой сегментации каждому полученному объекту, будь то абзац, предложение или \"подпредложение\", сопоставляется свой порядковый индекс; каждая следующая сегментация применяется для всех объектов полученной структуры."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re\n",
    "# texts -> paragraphs -> sentences -> words -> morph_parse\n",
    "\n",
    "# texts open\n",
    "open_text = lambda name: open(os.path.join(os.getcwd(), name), 'r').read()\n",
    "\n",
    "class Split:\n",
    "    def __init__(self, *names):\n",
    "        if len(names) == 0:\n",
    "            self.texts = None\n",
    "        elif len(names) == 1:\n",
    "            self.texts = {() : open_text(*names)}\n",
    "        else:\n",
    "            self.texts = {(i,) : text_i for i, text_i in enumerate(map(open_text, names))}\n",
    "        \n",
    "        # paragraph segmentation function\n",
    "        self.par_split = lambda text: re.split('[.!? ]*\\n+ {3} *', text)\n",
    "        self.par_segment = lambda: self.segment(self.texts, self.par_split)\n",
    "        # sentence segmentation function\n",
    "        self.sent_split = lambda text: re.split('[.!?]+ *', re.sub('\\n\\n+', '.', text))\n",
    "        self.sent_segment = lambda: self.segment(self.texts, self.sent_split)\n",
    "        # subsentence segmentation function\n",
    "        self.subsent_split = lambda text: re.split('[.!?:,]+ *', re.sub('\\n\\n+', '.', text))\n",
    "        self.subsent_segment = lambda: self.segment(self.texts, self.subsent_split)\n",
    "    \n",
    "    # segmentation function - segments all objects in self.texts\n",
    "    def segment(self, text, split_func):\n",
    "        self.texts = {key + (i,) : value_i for key, value in text.items()\n",
    "                     for i, value_i in enumerate(split_func(value))}\n",
    "        return sorted(self.texts.items())\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return str(self.texts)\n",
    "    def __getitem__(self, key):\n",
    "        result = sorted([(i, t_i) for i, t_i in self.texts.items() if i[:len(key)] == key])\n",
    "        return result[0][1] if len(result) == 1 else result\n",
    "    def __iter__(self):\n",
    "        return iter(sorted(self.texts.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0, 0) : Говард Ф\n",
      "(0, 0, 1) : Лавкрафт\n",
      "(0, 0, 2) : Музыка Эриха Цанна\n",
      "(0, 1, 0) : Я  самым внимательным образом изучил карты города, но так и  не отыскал\n",
      "на  них  улицу  д'Осейль\n",
      "(0, 1, 1) : Надо сказать, что  я  рылся отнюдь  не  только  в\n",
      "современных  картах,  поскольку мне  было  известно, что  подобные  названия\n",
      "нередко меняются\n",
      "(0, 1, 2) : Напротив, я, можно сказать,  по уши залез  в седую старину\n",
      "и, более того, лично обследовал интересовавший меня район, уже  не  особенно\n",
      "обращая  внимания на  таблички  и  вывески,  в  поисках  того,  что  хотя бы\n",
      "отдаленно походило на интересовавшую  меня улицу д'Осейль\n",
      "(0, 1, 3) : Однако,  несмотря\n",
      "на  все мои усилия, вынужден сейчас не без  стыда признаться,  что  так и не\n",
      "смог отыскать нужные мне дом, улицу, и даже приблизительно определить район,\n",
      "где,  в  течение  последних месяцев  моей  обездоленной  жизни,  я,  студент\n",
      "факультета метафизики, слушал музыку Эриха Занна\n",
      "(0, 2, 0) : Меня отнюдь не  удивляет  подобный провал в памяти, поскольку за период\n",
      "жизни на улице д'Осейль я серьезно подорвал как физическое, так и умственное\n",
      "здоровье,  и  потому  был   не  в  состоянии  вспомнить  ни  одного  из  тех\n",
      "немногочисленных знакомых, которые у меня там появились, Однако то, что я не\n",
      "могу припомнить  само это  место, кажется  мне  не  просто  странным,  но  и\n",
      "поистине  обескураживающим,  поскольку  располагалось  оно  не  далее, чем в\n",
      "получасе ходьбы от  университета, и было отмечено рядом весьма специфических\n",
      "особенностей, которые  едва  ли стерлись  бы  в  памяти любого, кто  хотя бы\n",
      "однажды там побывал\n",
      "(0, 3, 0) : И все  же  мне  ни разу не  довелось повстречать  человека, который  бы\n",
      "слышал про улицу д'Осейль\n",
      "(0, 4, 0) : По массивному, сложенному из  черного камня мосту  улица эта пересекала\n",
      "темную реку, вдоль которой располагались кирпичные стены складских помещений\n",
      "с  помутневшими  окнами\n",
      "....................................................................................................\n",
      "(1, 0, 0) : Говард Лавкрафт\n",
      "(1, 0, 1) : Дагон\n",
      "(1, 1, 0) : Я пишу  в состоянии  сильного душевного  напряжения,  поскольку сегодня\n",
      "ночью намереваюсь  уйти в  небытие\n",
      "(1, 1, 1) : Я нищ, а снадобье, единственно благодаря\n",
      "которому  течение моей жизни остается более или  менее  переносимым, уже  на\n",
      "исходе,  и  я  больше  не  могу  терпеть эту  пытку\n",
      "(1, 1, 2) : Поэтому мне ничего  не\n",
      "остается, кроме как выброситься вниз на грязную улицу из чердачного окна\n",
      "(1, 1, 3) : Не\n",
      "думайте,  что  я слабовольный  человек или  дегенерат, коль скоро нахожусь в\n",
      "рабской  зависимости от морфия\n",
      "(1, 1, 4) : Когда вы  прочтете эти написанные торопливой\n",
      "рукой  страницы, вы сможете  представить себе  хотя вам  не понять  этого до\n",
      "конца, как я дошел до состояния,  в котором смерть или забытье считаю лучшим\n",
      "для себя исходом\n",
      "(1, 2, 0) : Случилось так, что пакетбот, на котором я служил в качестве суперкарго,\n",
      "подвергся  нападению  немецкого  рейдера  в  одной из  наиболее пустынных  и\n",
      "наименее    посещаемых    кораблями    частей    Тихого   океана\n",
      "(1, 2, 1) : Большая\n",
      "война  в  то  время  только  начиналась,  и  океанская  флотилия\n",
      "гуннов еще  не погрязла  окончательно в  своих  пороках, как это\n",
      "случилось немного погодя\n",
      "(1, 2, 2) : Итак, наше судно стало законным военным трофеем, а\n",
      "с   нами,   членами   экипажа,   обращались   со  всей   обходительностью  и\n",
      "предупредительностью,  как и  подобает  обращаться  с  захваченными  в  плен\n",
      "моряками\n",
      "....................................................................................................\n"
     ]
    }
   ],
   "source": [
    "split = Split('text_1.txt', 'text_2.txt')\n",
    "split.par_segment()\n",
    "split.sent_segment()\n",
    "print_str = lambda items: '\\n'.join(map(lambda item: '{} : {}'.format(*item), items))\n",
    "print(print_str(split[(0,)][:10]), end = '\\n{}\\n'.format(100 * '.'))\n",
    "print(print_str(split[(1,)][:10]), end = '\\n{}\\n'.format(100 * '.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В приведенном выше примере мы можем видеть работу данного класса: мы подали на вход два текста, каждый из которых сначала разбили на абзацы, а затем на предложения. В результате мы получили список предложений с номерами вида (x, y, z), где x - номер текста, y - номер абзаца, z - номер предложения.\n",
    "\n",
    "Для удобства обращения с полученной структурой был переопределен метод __getitem__: по ключу он возвращает все объекты структуры, номера которых своим началом совпадают с этим ключом. Это позволяет работать с конкретным текстом, абзацем конкретного текста или предложением (по номерам вида (a,), (a, b), (a, b, c) соответственно)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно видеть недостаток \"наивной\" графематики: так, ее использование приводит к разделению на два предложения словосочетания \"Говард Ф.Лавкрафт\". В взятых нами текстах практически отсутствуют сокращения, поэтому мы будем довольствоваться и такой сегментацией."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Часть 2. Определение синтаксической связи двух слов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для определения синтаксической связи двух слов мы будем следовать определенным правилам высоковероятных связей, а значит нам потребуется использование морфоанализатора (мы не собираемся обрабатывать размеченный корпус текстов для автоматического определения правил, как не делали это и в случае с сегментацией).\n",
    "\n",
    "Для наших целей вполне подходит pymorphy2.MorphAnalyzer(): его метод parse() сопоставляет слову его возможные разборы; наибольший интерес представляют:\n",
    "* грамматические \"теги\" - набор грамем, - каждого разбора: именно они необходимы для применения каждого из правил;\n",
    "* нормальная форма слова: необходима для определения принадлежности к какой-либо группе (подробнее об этом будет сказано позже);\n",
    "* параметр <i>score</i>: мы будем стремиться к использованию информации обо всех возможных разборах, но тем не менее будем использовать его для оценки вероятности выполнения конкретного правила (то есть для снятия омонимии словосочетания - подробнее об этом будет сказано позже)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class OpencorporaTag in module pymorphy2.tagset:\n",
      "\n",
      "class OpencorporaTag(builtins.object)\n",
      " |  Wrapper class for OpenCorpora.org tags.\n",
      " |  \n",
      " |  .. warning::\n",
      " |  \n",
      " |      In order to work properly, the class has to be globally\n",
      " |      initialized with actual grammemes (using _init_grammemes method).\n",
      " |  \n",
      " |      Pymorphy2 initializes it when loading a dictionary;\n",
      " |      it may be not a good idea to use this class directly.\n",
      " |      If possible, use ``morph_analyzer.TagClass`` instead.\n",
      " |  \n",
      " |  Example::\n",
      " |  \n",
      " |      >>> from pymorphy2 import MorphAnalyzer\n",
      " |      >>> morph = MorphAnalyzer()\n",
      " |      >>> Tag = morph.TagClass  # get an initialzed Tag class\n",
      " |      >>> tag = Tag('VERB,perf,tran plur,impr,excl')\n",
      " |      >>> tag\n",
      " |      OpencorporaTag('VERB,perf,tran plur,impr,excl')\n",
      " |  \n",
      " |  Tag instances have attributes for accessing grammemes::\n",
      " |  \n",
      " |      >>> print(tag.POS)\n",
      " |      VERB\n",
      " |      >>> print(tag.number)\n",
      " |      plur\n",
      " |      >>> print(tag.case)\n",
      " |      None\n",
      " |  \n",
      " |  Available attributes are: POS, animacy, aspect, case, gender, involvement,\n",
      " |  mood, number, person, tense, transitivity and voice.\n",
      " |  \n",
      " |  You may check if a grammeme is in tag or if all grammemes\n",
      " |  from a given set are in tag::\n",
      " |  \n",
      " |      >>> 'perf' in tag\n",
      " |      True\n",
      " |      >>> 'nomn' in tag\n",
      " |      False\n",
      " |      >>> 'Geox' in tag\n",
      " |      False\n",
      " |      >>> set(['VERB', 'perf']) in tag\n",
      " |      True\n",
      " |      >>> set(['VERB', 'perf', 'sing']) in tag\n",
      " |      False\n",
      " |  \n",
      " |  In order to fight typos, for unknown grammemes an exception is raised::\n",
      " |  \n",
      " |      >>> 'foobar' in tag\n",
      " |      Traceback (most recent call last):\n",
      " |      ...\n",
      " |      ValueError: Grammeme is unknown: foobar\n",
      " |      >>> set(['NOUN', 'foo', 'bar']) in tag\n",
      " |      Traceback (most recent call last):\n",
      " |      ...\n",
      " |      ValueError: Grammemes are unknown: {'bar', 'foo'}\n",
      " |  \n",
      " |  This also works for attributes::\n",
      " |  \n",
      " |      >>> tag.POS == 'plur'\n",
      " |      Traceback (most recent call last):\n",
      " |      ...\n",
      " |      ValueError: 'plur' is not a valid grammeme for this attribute. Valid grammemes: ...\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  POS\n",
      " |      Descriptor object for accessing grammemes of certain classes\n",
      " |      (e.g. number or voice).\n",
      " |  \n",
      " |  __contains__(self, grammeme)\n",
      " |  \n",
      " |  __eq__(self, other)\n",
      " |      Return self==value.\n",
      " |  \n",
      " |  __gt__(self, other)\n",
      " |      Return self>value.\n",
      " |  \n",
      " |  __hash__(self)\n",
      " |      Return hash(self).\n",
      " |  \n",
      " |  __init__(self, tag)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __len__(self)\n",
      " |  \n",
      " |  __lt__(self, other)\n",
      " |      Return self<value.\n",
      " |  \n",
      " |  __ne__(self, other)\n",
      " |      Return self!=value.\n",
      " |  \n",
      " |  __reduce__(self)\n",
      " |      helper for pickle\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __str__(self)\n",
      " |      Return str(self).\n",
      " |  \n",
      " |  animacy\n",
      " |      Descriptor object for accessing grammemes of certain classes\n",
      " |      (e.g. number or voice).\n",
      " |  \n",
      " |  aspect\n",
      " |      Descriptor object for accessing grammemes of certain classes\n",
      " |      (e.g. number or voice).\n",
      " |  \n",
      " |  case\n",
      " |      Descriptor object for accessing grammemes of certain classes\n",
      " |      (e.g. number or voice).\n",
      " |  \n",
      " |  gender\n",
      " |      Descriptor object for accessing grammemes of certain classes\n",
      " |      (e.g. number or voice).\n",
      " |  \n",
      " |  involvement\n",
      " |      Descriptor object for accessing grammemes of certain classes\n",
      " |      (e.g. number or voice).\n",
      " |  \n",
      " |  is_productive(self)\n",
      " |  \n",
      " |  mood\n",
      " |      Descriptor object for accessing grammemes of certain classes\n",
      " |      (e.g. number or voice).\n",
      " |  \n",
      " |  number\n",
      " |      Descriptor object for accessing grammemes of certain classes\n",
      " |      (e.g. number or voice).\n",
      " |  \n",
      " |  numeral_agreement_grammemes(self, num)\n",
      " |  \n",
      " |  person\n",
      " |      Descriptor object for accessing grammemes of certain classes\n",
      " |      (e.g. number or voice).\n",
      " |  \n",
      " |  tense\n",
      " |      Descriptor object for accessing grammemes of certain classes\n",
      " |      (e.g. number or voice).\n",
      " |  \n",
      " |  transitivity\n",
      " |      Descriptor object for accessing grammemes of certain classes\n",
      " |      (e.g. number or voice).\n",
      " |  \n",
      " |  updated_grammemes(self, required)\n",
      " |      Return a new set of grammemes with ``required`` grammemes added\n",
      " |      and incompatible grammemes removed.\n",
      " |  \n",
      " |  voice\n",
      " |      Descriptor object for accessing grammemes of certain classes\n",
      " |      (e.g. number or voice).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  add_grammemes_to_known(lat, cyr) from builtins.type\n",
      " |  \n",
      " |  cyr2lat(tag_or_grammeme) from builtins.type\n",
      " |      Return Latin representation for ``tag_or_grammeme`` string\n",
      " |  \n",
      " |  fix_rare_cases(grammemes) from builtins.type\n",
      " |      Replace rare cases (loc2/voct/...) with common ones (loct/nomn/...).\n",
      " |  \n",
      " |  grammeme_is_known(grammeme) from builtins.type\n",
      " |  \n",
      " |  lat2cyr(tag_or_grammeme) from builtins.type\n",
      " |      Return Cyrillic representation for ``tag_or_grammeme`` string\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  cyr_repr\n",
      " |      Cyrillic representation of this tag\n",
      " |  \n",
      " |  grammemes\n",
      " |      A frozenset with grammemes for this tag.\n",
      " |  \n",
      " |  grammemes_cyr\n",
      " |      A frozenset with Cyrillic grammemes for this tag.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  ANIMACY = frozenset({'anim', 'inan'})\n",
      " |  \n",
      " |  ASPECTS = frozenset({'impf', 'perf'})\n",
      " |  \n",
      " |  CASES = frozenset({'ablt', 'acc2', 'accs', 'datv', 'gen1', 'gen2', ......\n",
      " |  \n",
      " |  FORMAT = 'opencorpora-int'\n",
      " |  \n",
      " |  GENDERS = frozenset({'femn', 'masc', 'neut'})\n",
      " |  \n",
      " |  INVOLVEMENT = frozenset({'excl', 'incl'})\n",
      " |  \n",
      " |  KNOWN_GRAMMEMES = {'1per', '2per', '3per', 'ADJF', 'ADJS', 'ADVB', ......\n",
      " |  \n",
      " |  MOODS = frozenset({'impr', 'indc'})\n",
      " |  \n",
      " |  NUMBERS = frozenset({'plur', 'sing'})\n",
      " |  \n",
      " |  PARTS_OF_SPEECH = frozenset({'ADJF', 'ADJS', 'ADVB', 'COMP', 'CONJ', '...\n",
      " |  \n",
      " |  PERSONS = frozenset({'1per', '2per', '3per'})\n",
      " |  \n",
      " |  RARE_CASES = {'acc1': 'accs', 'acc2': 'accs', 'gen1': 'gent', 'gen2': ...\n",
      " |  \n",
      " |  TENSES = frozenset({'futr', 'past', 'pres'})\n",
      " |  \n",
      " |  TRANSITIVITY = frozenset({'intr', 'tran'})\n",
      " |  \n",
      " |  VOICES = frozenset({'actv', 'pssv'})\n",
      " |  \n",
      " |  typed_grammemes = True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pymorphy2\n",
    "# useful links:\n",
    "# https://pymorphy2.readthedocs.io/en/latest/user/guide.html\n",
    "# https://pymorphy2.readthedocs.io/en/latest/user/grammemes.html#russian-genders\n",
    "# http://opencorpora.org/dict.php?act=gram\n",
    "help(pymorphy2.MorphAnalyzer().TagClass)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для дальнейшей работы необходимо разобраться в системе тегов, реализованных классом pymorphy2.MorphAnalyzer().TagClass на основе тегов OpenCorpora и отличия от них.\n",
    "\n",
    "Одно из отличий, например, прописано в документации:  \n",
    "\" В OpenCorpora (на июль 2013) есть еще падежи gen1 и loc1.\n",
    "Они указываются вместо gent/loct, когда у слова есть форма gen2/loc2. \n",
    "В pymorphy2 gen1 и loc1 заменены на gent/loct, чтоб с ними было проще работать. \"\n",
    "\n",
    "В целом, нас будут интересовать не отличия от OpenCorpora, а особенности разбора слов с использованием pymorphy2.MorphAnalyzer().\n",
    "\n",
    "Ниже дан список тегов, которые могут быть нам полезны, с их возможными значениями с их кирилличесским вариантом для большей ясности."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANIMACY - .animacy(), одушевленность:\n",
      "[('inan', 'неод'), ('anim', 'од')]\n",
      "\n",
      "INVOLVEMENT - .involvement(), включенность говорящего в действие:\n",
      "[('incl', 'вкл'), ('excl', 'выкл')]\n",
      "\n",
      "NUMBERS - .number(), число (единственное, множественное):\n",
      "[('plur', 'мн'), ('sing', 'ед')]\n",
      "\n",
      "CASES - .case(), падеж:\n",
      "[('loc2', 'пр2'), ('loc1', 'пр1'), ('nomn', 'им'), ('acc2', 'вн2'), ('accs', 'вн'), ('voct', 'зв'), ('gent', 'рд'), ('ablt', 'тв'), ('gen2', 'рд2'), ('gen1', 'рд1'), ('loct', 'пр'), ('datv', 'дт')]\n",
      "\n",
      "TENSES - .tense(), время (настоящее, прошедшее, будущее):\n",
      "[('pres', 'наст'), ('futr', 'буд'), ('past', 'прош')]\n",
      "\n",
      "PARTS_OF_SPEECH - .POS(), часть речи:\n",
      "[('PRTF', 'ПРИЧ'), ('INFN', 'ИНФ'), ('ADVB', 'Н'), ('VERB', 'ГЛ'), ('NOUN', 'СУЩ'), ('GRND', 'ДЕЕПР'), ('NUMR', 'ЧИСЛ'), ('PRED', 'ПРЕДК'), ('ADJS', 'КР_ПРИЛ'), ('PRTS', 'КР_ПРИЧ'), ('COMP', 'КОМП'), ('INTJ', 'МЕЖД'), ('PREP', 'ПР'), ('NPRO', 'МС'), ('CONJ', 'СОЮЗ'), ('ADJF', 'ПРИЛ'), ('PRCL', 'ЧАСТ')]\n",
      "\n",
      "MOODS - .mood(), наклонение (повелительное, изъявительное):\n",
      "[('impr', 'повел'), ('indc', 'изъяв')]\n",
      "\n",
      "PERSONS - .person(), лицо (1, 2, 3):\n",
      "[('2per', '2л'), ('1per', '1л'), ('3per', '3л')]\n",
      "\n",
      "VOICES - voice, залог (действительный, страдательный):\n",
      "[('pssv', 'страд'), ('actv', 'действ')]\n",
      "\n",
      "GENDERS - .gender(), род (мужской, женский, средний):\n",
      "[('femn', 'жр'), ('neut', 'ср'), ('masc', 'мр')]\n",
      "\n",
      "ASPECTS - .aspect(), вид (совершенный или несовершенный):\n",
      "[('impf', 'несов'), ('perf', 'сов')]\n",
      "\n",
      "TRANSITIVITY - .transitivity(), переходность (переходный, непереходный):\n",
      "[('intr', 'неперех'), ('tran', 'перех')]\n"
     ]
    }
   ],
   "source": [
    "morph_analyzer = pymorphy2.MorphAnalyzer()\n",
    "meaningful_tags = {'ANIMACY' : '.animacy(), одушевленность',\n",
    "                   'ASPECTS' : '.aspect(), вид (совершенный или несовершенный)',\n",
    "                   'CASES' : '.case(), падеж',\n",
    "                   'GENDERS' : '.gender(), род (мужской, женский, средний)',\n",
    "                   'INVOLVEMENT' : '.involvement(), включенность говорящего в действие',\n",
    "                   'MOODS' : '.mood(), наклонение (повелительное, изъявительное)',\n",
    "                   'NUMBERS' : '.number(), число (единственное, множественное)',\n",
    "                   'PARTS_OF_SPEECH' : '.POS(), часть речи',\n",
    "                   'PERSONS' : '.person(), лицо (1, 2, 3)',\n",
    "                   'TENSES' : '.tense(), время (настоящее, прошедшее, будущее)',\n",
    "                   'TRANSITIVITY' : '.transitivity(), переходность (переходный, непереходный)',\n",
    "                   'VOICES' : 'voice, залог (действительный, страдательный)'}\n",
    "pos_lat_tags = lambda tag: list(morph_analyzer.TagClass.__dict__[tag])\n",
    "pos_cyr_tags = lambda tag: [morph_analyzer.lat2cyr(t) for t in pos_lat_tags(tag)]\n",
    "print('\\n\\n'.join(['{} - {}:\\n{}'.format(tag, desc, list(zip(pos_lat_tags(tag), pos_cyr_tags(tag))))\n",
    "                   for tag, desc in meaningful_tags.items()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, как уже было сказано выше, реализация правил сопряжена с пониманием особенностью разбора каждого из слов в анализируемой паре.  \n",
    "Ниже будут продемонстрированы некоторые из ключевых особенностей, которыми мы будем пользоваться (или будем с ними мириться)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Список ключевых (для нашей задачи) особенностей pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Неизменяемым существительным и прилагательными сопоставляются разборы со всеми возможные падежи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "падежи неизменяемого существительного 'пальто':\n",
      "nomn, accs, gent, ablt, loct, datv\n",
      "\n",
      "падежи неизменяемого прилагательного 'электрик':\n",
      "nomn, accs, gent, ablt, loct, datv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "word_tags = lambda word, tag: \"'{}':\\n{}\".format(word, ', '.join({str(getattr(var.tag, tag))\n",
    "                                                                  for var in morph_analyzer.parse(word)}))\n",
    "print('падежи неизменяемого существительного ' + word_tags('пальто', 'case'), end = '\\n\\n')\n",
    "print('падежи неизменяемого прилагательного ' + word_tags('электрик', 'case'), end = '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы будем пользоваться этим при определении согласования: отпадает необходимость проверять \"неизменяемость\" слов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Прилагательному во множественной форме в разборах не сопоставляется род (в отличие от существительных)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "прилагательное во множественной форме 'неприятные':\n",
      "неприятный + (ADJF,Qual plur,nomn)\n",
      "неприятный + (ADJF,Qual inan,plur,accs)\n",
      "\n",
      "существительное во множественной форме 'проблемы':\n",
      "проблема + (NOUN,inan,femn sing,gent)\n",
      "проблема + (NOUN,inan,femn plur,nomn)\n",
      "проблема + (NOUN,inan,femn plur,accs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "word_parse = lambda word: \"'{}':\\n{}\".format(word, '\\n'.join(map(lambda p: '{} + ({})'.format(p.normal_form, p.tag),\n",
    "                                                                 morph_analyzer.parse(word))))\n",
    "print('прилагательное во множественной форме ' + word_parse('неприятные'), end = '\\n\\n')\n",
    "print('существительное во множественной форме ' + word_parse('проблемы'), end = '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Об этом также важно помнить при определении согласования: если в единственном числе мы требуем совпадения по падежу, числу и роду (case, number & gender), то во множественном совпадение должно быть только по числу и падежу."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Отглагольному прилагательному сопоставляется разбор, в котором оно является причастием (а нормальная форма - глагол)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "отглагольное прилагательное 'открытый':\n",
      "открытый + (ADJF,Qual masc,sing,nomn)\n",
      "открытый + (ADJF,Qual inan,masc,sing,accs)\n",
      "открыть + (PRTF,perf,tran,past,pssv masc,sing,nomn)\n",
      "открыть + (PRTF,perf,tran,past,pssv inan,masc,sing,accs)\n",
      "\n",
      "отглагольное существительное 'освобождение':\n",
      "освобождение + (NOUN,inan,neut sing,nomn)\n",
      "освобождение + (NOUN,inan,neut sing,accs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('отглагольное прилагательное ' + word_parse('открытый'), end = '\\n\\n')\n",
    "# с отглагольным существительным такого не происходит\n",
    "print('отглагольное существительное ' + word_parse('освобождение'), end = '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это свойство будет использоваться при определении отглагольности прилагательного; к сожалению, определить отглагольность существительного с помощью одного лишь морфоанализатора вызывает затруднения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://ru.wikipedia.org/wiki/Отглагольное_существительное  \n",
    "https://ru.wiktionary.org/wiki/Категория:Русские_отглагольные_существительные  \n",
    "https://ru.wiktionary.org/wiki/-ациj  \n",
    "https://ru.wiktionary.org/wiki/-тель\n",
    "\n",
    "Воспользовавшись данными ссылками можно узнать про самый популярный способ образования отглагольных существительных:\n",
    "* при помощи суффикса -ни-е (-ани-е, -ени-е, -яни-е);\n",
    "* при помощи суффикса -аци-я (при добавлении к основе глаголов с иноязычной основой);\n",
    "* при помощи суффикса -тель (при добавлении к основе неопределённой формы глагола);\n",
    "\n",
    "а также ознакомиться с другими способами их образования. Можно видеть, что приведенные способы образования дают наибольшее количество примеров подобных существительных: этот факт и можно было бы использовать в качестве правила, а для прочих \"редких\" форм образования составить словарь отглагольных существительных.\n",
    "\n",
    "Но, конечно же, есть исключения:\n",
    "* зрение, сознание (происходят от глагола, но потеряло свою отглагольность), поколение (от праслав. *kolěno, корень -колен-)\n",
    "* метель (происходит от глагола, но потеряло свою отглагольность), картель (от фр. cartel — «вызов на поединок»)\n",
    "* нация (от лат. natio - племя, народ)\n",
    "\n",
    "Поэтому составленное по описанному выше принципу правило будет работать в большом количестве случаев, но не гарантирует верного результата. Тем не менее, нами будет использоваться именно такое правило, с добавлением словаря \"редких\" отглагольных существительных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Глаголу в нормальной форме (то есть инфинитиву) сопоставляется отличная от VERB часть речи - а именно INFΝ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "глагол 'играю':\n",
      "играть + (VERB,impf,tran sing,1per,pres,indc)\n",
      "\n",
      "глагол в нормальной форме (инфинитив) 'играть':\n",
      "играть + (INFN,impf,tran)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('глагол ' + word_parse('играю'), end = '\\n\\n')\n",
    "print('глагол в нормальной форме (инфинитив) ' + word_parse('играть'), end = '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это необходимо учитывать в формулировке правил, в которых участвует глагол в качестве главного слова и связь сохраняется при постановке его в нормальную форму."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Такие разные местоимения.\n",
    "\n",
    "Посмотрим, какие разборы сопоставляются местоимениям:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "личное местоимение 'нас':\n",
      "мы + (NPRO,1per plur,gent)\n",
      "мы + (NPRO,1per plur,accs)\n",
      "мы + (NPRO,1per plur,loct)\n",
      "\n",
      "относительное местоимение 'кого':\n",
      "кто + (NPRO,masc sing,accs)\n",
      "кто + (NPRO,masc sing,gent)\n",
      "относительное местоимение 'который':\n",
      "который + (ADJF,Apro,Subx,Anph masc,sing,nomn)\n",
      "который + (ADJF,Apro,Subx,Anph inan,masc,sing,accs)\n",
      "\n",
      "указательное местоимение 'стольких':\n",
      "столько + (NUMR gent)\n",
      "столько + (NUMR anim,accs)\n",
      "столько + (NUMR loct)\n",
      "указательное местоимение 'таков':\n",
      "таков + (ADJS,Apro masc,sing)\n",
      "\n",
      "определительное местоимение 'всякому':\n",
      "всякий + (ADJF,Apro masc,sing,datv)\n",
      "всякий + (ADJF,Apro neut,sing,datv)\n",
      "всякий + (NPRO,masc sing,datv)\n",
      "всякое + (NPRO,neut sing,datv)\n",
      "\n",
      "отрицательное местоимение 'никому':\n",
      "никто + (NPRO sing,datv)\n",
      "отрицательное местоимение 'ничьему':\n",
      "ничей + (ADJF,Apro masc,sing,datv)\n",
      "ничей + (ADJF,Apro neut,sing,datv)\n",
      "\n",
      "неопределенное местоимение 'несколько':\n",
      "несколько + (ADVB)\n",
      "несколько + (NUMR nomn)\n",
      "несколько + (NUMR inan,accs)\n",
      "неопределенное местоимение 'кое-кто':\n",
      "кое-кто + (NPRO,masc sing,nomn)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('личное местоимение ' + word_parse('нас'), end = '\\n\\n')\n",
    "print('относительное местоимение ' + word_parse('кого'), end = '\\n')\n",
    "print('относительное местоимение ' + word_parse('который'), end = '\\n\\n')\n",
    "print('указательное местоимение ' + word_parse('стольких'), end = '\\n')\n",
    "print('указательное местоимение ' + word_parse('таков'), end = '\\n\\n')\n",
    "print('определительное местоимение ' + word_parse('всякому'), end = '\\n\\n')\n",
    "print('отрицательное местоимение ' + word_parse('никому'), end = '\\n')\n",
    "print('отрицательное местоимение ' + word_parse('ничьему'), end = '\\n\\n')\n",
    "print('неопределенное местоимение ' + word_parse('несколько'), end = '\\n')\n",
    "print('неопределенное местоимение ' + word_parse('кое-кто'), end = '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно видеть, что далеко не всем местоимениям сопоставляется часть речи 'NPRO'; тем не менее, всем местоимениям, что изменяются по родам, числам и падежам и согласуются с существительным, сопоставлены их число, падеж и род; в случае склонения только по падежам в разборах прописан падеж. Из-за таких разных и множественных интерпретаций трудно добавить обработку местоимений - тем более, некоторые из них (те, что имеют в разборах часть речи, которой они могут соответствовать).\n",
    "\n",
    "Отметим лишь тот факт, что морфоанализатор может ставить местоимениям в качестве части речи: NPRO, ADJF, ADJS, NUMR, но никак не NOUN. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ухищрения в дополнение к использованию морфоанализатора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'среди':\n",
      "среди + (PREP)\n",
      "'между':\n",
      "между + (PREP)\n"
     ]
    }
   ],
   "source": [
    "print(word_parse('среди'))\n",
    "print(word_parse('между'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ранее мы пришли к выводу, что нам необходима дополнительная информация об отглагольных существительных. То же самое верно и для предлогов, поэтому нам необходим словарь соответствий предлогов и обслуживаемых ими падежей.\n",
    "\n",
    "Поэтому мы воспользовались ссылкой https://ru.wiktionary.org/wiki/Категория:Русские_предлоги и составили словарь предлогов, употребляющихся с родительным, дательным, винительным, творительным и предложным падежами\n",
    "\n",
    "Для того, чтобы понять, какие предлоги обслуживают второй родительный, второй винительный и второй предложный, мы воспользовались ссылками http://rusgram.ru/Падеж и http://rusgram.ru/Предложный_падеж#12 , где говорится следующее:\n",
    "* ...Второй винительный падеж (другие названия – включительный, превратительный, собирательный) встречается после предлога в при небольшом количестве глаголов...\n",
    "*  ...У некоторых существительных после локативных предлогов в и на употребляется особая форма предложного падежа...  \n",
    "...локативная форма употребляется не со всеми предлогами, управляющими предложным падежом: встречается после в и на, но не встречается после о, при, по;\n",
    "\n",
    "Для второго родительного падежа списка употребляемых с ним предлогов не дано, но даны примеры словосочетаний с ним:\n",
    "* ложка сахару; чашка чаю; Народу набежало!; Шуму было!\n",
    "* ни разу; выпьем чайку;\n",
    "* без году неделя; нашего полку прибыло; с миру по нитке; моя хата с краю; беситься с жиру и др.\n",
    "\n",
    "В тексте статьи говорится, что хоть второй родительный широко используется в разговорной речи, но обязательным является лишь в отдельных случаях, которые по большей части являются фразеологизмами. Из этого нами был сделан вывод, что будет достаточно добавить в предлоги, которые обслуживают второй родительный падеж, только <i>без</i> и <i>с</i>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gent :\n",
      "['без', 'без ведома', 'безо', 'близ', 'близко от', 'в виде', 'в зависимости от', 'в качестве', 'в лице', 'в отличие от', 'в отношении', 'в пандан', 'в преддверии', 'в продолжение', 'в результате', 'в роли', 'в силу', 'в течение', 'в целях', 'вблизи', 'ввиду', 'вглубь', 'вдоль', 'взамен', 'вместо', 'вне', 'внизу', 'внутри', 'внутрь', 'во благо', 'вовнутрь', 'возле', 'вокруг', 'впереди', 'впредь до', 'вроде', 'вследствие', 'для', 'до', 'за вычетом', 'за исключением', 'за счёт', 'заместо', 'из', 'из-за', 'из-под', 'изнутри', 'изо', 'исходя из', 'кроме', 'кругом', 'меж', 'между', 'мимо', 'на благо', 'на виду у', 'на глазах у', 'на предмет', 'наверху', 'накануне', 'наподобие', 'напротив', 'насупротив', 'насчёт', 'начиная с', 'не без', 'не считая', 'недалеко от', 'независимо от', 'ниже', 'обок', 'около', 'окромя', 'округ', 'от', 'от имени', 'от лица', 'относительно', 'ото', 'по линии', 'по мере', 'по поводу', 'по причине', 'по случаю', 'поблизости от', 'поверх', 'под видом', 'под эгидой', 'подле', 'позади', 'помимо', 'поперёк', 'посередине', 'посередь', 'после', 'посреди', 'посредине', 'посредством', 'прежде', 'при помощи', 'против', 'путём', 'ради', 'с', 'с ведома', 'с помощью', 'с точки зрения', 'с целью', 'сверх', 'свыше', 'сзади', 'снизу', 'со', 'среди', 'средь', 'супротив', 'у']\n",
      "gen2 :\n",
      "['без', 'с']\n",
      "datv :\n",
      "['благодаря', 'в пандан', 'вдоль по', 'во благо', 'вопреки', 'вслед', 'к', 'ко', 'лицом к', 'на благо', 'навстречу', 'наперекор', 'наперерез', 'по', 'по направлению к', 'по отношению к', 'подобно', 'применительно к', 'смотря по', 'согласно', 'сродни', 'судя по']\n",
      "accs :\n",
      "['в', 'включая', 'во', 'за', 'на', 'невзирая на', 'несмотря на', 'о', 'об', 'обо', 'по', 'по-за', 'под', 'подо', 'про', 'с', 'с прицелом на', 'сквозь', 'со', 'спустя', 'через', 'чрез']\n",
      "acc2 :\n",
      "['в']\n",
      "ablt :\n",
      "['в связи с', 'в соответствии с', 'вслед за', 'за', 'лицом к лицу с', 'меж', 'между', 'над', 'надо', 'наряду с', 'перед', 'передо', 'по сравнению с', 'по-над', 'под', 'подо', 'пред', 'предо', 'рядом с', 'с', 'следом за', 'со']\n",
      "loct :\n",
      "['в', 'во', 'на', 'о', 'об', 'обо', 'по', 'при']\n",
      "loc2 :\n",
      "['в', 'на']\n"
     ]
    }
   ],
   "source": [
    "for preps_str in re.split('\\n\\_+[\\w\\-]*\\_+\\n', open_text('preps&cases.txt'))[1:]:\n",
    "    print((lambda p: '{} :\\n{}'.format(p[0], p[1:]))(re.split('\\n', preps_str)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим класс <b>BigramParser</b>, который реализует определение синтаксической связи двух слов при помощи метода connect, который по двум словам (порядок которых важен) возвращает список вида <b>[((главное слово, тип синтаксической связи), score)]</b>, упорядоченный по параметру <i>score</i>, где <i>score</i> - это \"вероятность\" данного разбора.\n",
    "\n",
    "Для каждого типа связи есть правило, которое подразумевает определенный набор характеристик для каждого из слов: в общем случае одной из характеристик является часть речи первого и второго слова, и эти характеристики проверяются в первую очередь, но в ряде случаев этого недостаточно:\n",
    "* согласование A|P -> N предполагает совпадающий число, падеж и род (в случае ед.ч.);\n",
    "* зависимость Prep -> N предполагает соответствие падежа существительного падежам, обслуживаемым предлогом;\n",
    "* зависимость N -> Prep предполагает отглагольность существительного;\n",
    "* зависимость N -> N предполагает родительный падеж (или второй родительный, например: ложка сахару, чашка чаю) второго существительного;\n",
    "* зависимость V -> N предполагает винительный падеж существительного (но не второй винительный, поскольку он \"встречается после предлога в при небольшом количестве глаголов\", например: [пойти, записаться, выбиться,...] в [солдаты,летчики,начальники])\n",
    "\n",
    "Как можно видеть, большинство характеристик касаются варианта разбора одного из слов (помимо отглагольности существительного, которая определяется нами \"вручную\"). Мы устанавливаем связь между двумя словами, если в их морфологических разборах присутствует такие разборы, что они удовлетворяют заданным нами характеристикам. Для каждого из слов мы можем найти все подходящие под эти характеристики разборы и посчитать \"вероятность соответствия\" этим характеристикам - она будет определяться, как сумма соответсвующих этим разборам параметров <i>score</i>. Параметр же <i>score</i> для всей связи вычисляется как произведение полученных суммарных вероятностей, то есть как вероятность двух независимых событий: соответствия каждого из слов своим характеристикам."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перечислим список реализованных вспомогательных методов:\n",
    "* morph_parse: возвращает набор всех возможных морфологических разборов слова;\n",
    "* normal_forms: возвращает набор всех возможных нормальных форм слова;\n",
    "* tag_scores: возвращает словарь вида {тег разбора : его вероятность}\n",
    "* check_tag: по слову и списку значений тегов возвращает наличие хотя бы одного из значений хотя бы в одном из разборов;\n",
    "* morph_tags: по слову и списку тегов возвращает значения этих тегов для каждого разбора;\n",
    "* dicts_init: загружает словари \"редких\" отглагольных существительных и предлогов для каждого из падежей + создает словарь вида {предлог : список обслуживаемых падежей}\n",
    "\n",
    "Также в методе connect активно используется функция check_POSes, которая по двум наборам частей речи проверяет, не пусто ли пересечение каждого из наборов с возможными частями речи каждого из слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preposition to cases:\n",
      "под + ablt, accs\n",
      "в течение + gent\n",
      "благодаря + datv\n",
      "подле + gent\n",
      "насупротив + gent\n",
      "по мере + gent\n",
      "против + gent\n",
      "сзади + gent\n",
      "из-за + gent\n",
      "смотря по + datv\n",
      "на + loct, loc2, accs\n",
      "впереди + gent\n",
      "надо + ablt\n",
      "на глазах у + gent\n",
      "наперекор + datv\n",
      "из + gent\n",
      "изо + gent\n",
      "ради + gent\n",
      "меж + gent, ablt\n",
      "по сравнению с + ablt\n",
      "...\n",
      "\n",
      "verbal nouns:\n",
      "скидка, загар, отток, рукоприкладство, тяжба, сдвиг, задержка, ходьба, ловля, пересмотр, воображала, отсрочка, налёт, грызня, ставка, отправка, обжимка, огрызок, беготня, убежище, помолвка, заход, вышивка, выписка, подарок, варка, выход, битва\n"
     ]
    }
   ],
   "source": [
    "class BigramParser:\n",
    "    def __init__(self):\n",
    "        # morphological analyzer\n",
    "        self.morph_analyzer = pymorphy2.MorphAnalyzer()\n",
    "        \n",
    "        self.morph_parse = lambda word: self.morph_analyzer.parse(word)\n",
    "        \n",
    "        self.normal_forms = lambda word: {var.normal_form for var in self.morph_parse(word)}\n",
    "        \n",
    "        self.tag_scores = lambda word: {var.tag : var.score for var in self.morph_parse(word)}\n",
    "        \n",
    "        self.check_tag = lambda word, *tag_vals: any(t in var for var in self.tag_scores(word) for t in tag_vals)\n",
    "        \n",
    "        self.morph_tags = lambda word, *tags: {','.join((str(getattr(var.tag, t)) for t in tags))\n",
    "                                               for var in self.morph_parse(word)}\n",
    "        \n",
    "        self.dicts_init()\n",
    "    \n",
    "    def dicts_init(self, case_preps_txt = 'preps&cases.txt', verbal_nouns_txt = 'verbal_nouns.txt'):\n",
    "        self.case_preps = dict((lambda p: (p[0], p[1:]))(re.split('\\n', preps))\n",
    "                                for preps in re.split('\\n\\_+[\\w\\-]*\\_+\\n', open_text(case_preps_txt))[1:])\n",
    "        self.prep_cases = {p : {c for c in self.case_preps if p in self.case_preps[c]}\n",
    "                            for c in self.case_preps for p in self.case_preps[c]}\n",
    "        self.verbal_nouns = set(re.split('\\n', open_text(verbal_nouns_txt)))\n",
    "    \n",
    "    def connect(self, first, second):\n",
    "        POSes = dict(zip(('first', 'second'), map(lambda word: set(self.morph_tags(word, 'POS')), (first, second))))\n",
    "        check_POSes = lambda p_1, p_2: set(p_1.split(','))&POSes['first'] and set(p_2.split(','))&POSes['second']\n",
    "        \n",
    "        pos_connects = {}\n",
    "        score = lambda word, *pos_tags: sum(score for tag, score in self.tag_scores(word).items() for t in pos_tags\n",
    "                                            if 'None' not in t and set(t.split(',')) in tag)\n",
    "        \n",
    "        if check_POSes('ADJF,NUMR,PRTF', 'NOUN'):\n",
    "            POS_group = POSes['first']&{'ADJF', 'NUMR', 'PRTF'}\n",
    "            sing_tag_group = {'case', 'number', 'gender'}\n",
    "            plur_tag_group = {'case', 'number'}\n",
    "            if POS_group:\n",
    "                sing_tags = self.morph_tags(first, *sing_tag_group)&self.morph_tags(second, *sing_tag_group)\n",
    "                plur_tags = self.morph_tags(first, *plur_tag_group)&self.morph_tags(second, *plur_tag_group)\n",
    "                concord_tags = plur_tags if self.check_tag(first, 'plur') and self.check_tag(second, 'plur') else sing_tags\n",
    "                # concord : {ADJF,NUMR,PRTF} <- NOUN\n",
    "                if concord_tags:\n",
    "                    first_score = score(first, *['{},{}'.format(POS, case) for POS in POS_group for case in concord_tags])\n",
    "                    second_score = score(second, *['{},{}'.format('NOUN', case) for case in concord_tags])\n",
    "                    pos_connects[(second, '{} <- NOUN'.format('|'.join(POS_group)))] = first_score*second_score\n",
    "                # PRTF -> NOUN\n",
    "                elif 'PRTF' in POSes['first']:\n",
    "                    pos_connects[(first, 'PRTF -> NOUN')] = score(first, 'PRTF')*score(second, 'NOUN')\n",
    "\n",
    "        # PREP -> NOUN\n",
    "        if check_POSes('PREP', 'NOUN'):\n",
    "            if first.lower() in self.prep_cases and self.prep_cases[first.lower()]&self.morph_tags(second, 'case'):\n",
    "                pos_connects[(first, 'PREP -> NOUN')] = score(first, 'PREP')*score(second, 'NOUN')\n",
    "\n",
    "        # NOUN -> PREP   \n",
    "        if check_POSes('NOUN', 'PREP'):\n",
    "            if first[-4:-1] in {'ани', 'ени', 'яни', 'аци', 'тел'} or self.normal_forms(first)&self.verbal_nouns:\n",
    "                pos_connects[(first, 'NOUN -> PREP')] = score(first, 'NOUN')*score(second, 'PREP')\n",
    "\n",
    "        # NOUN -> NOUN + gent (or gen2)\n",
    "        if check_POSes('NOUN', 'NOUN') and self.check_tag(second, 'gent', 'gen2'):\n",
    "            pos_connects[(first, 'NOUN -> NOUN')] = score(first, 'NOUN')*score(second, 'NOUN,gent', 'NOUN,gen2')\n",
    "        \n",
    "        # {VERB,INFN} -> NOUN + accs\n",
    "        if check_POSes('VERB,INFN', 'NOUN') and self.check_tag(second, 'accs'):\n",
    "            POS_group = POSes['first']&{'VERB', 'INFN'}\n",
    "            first_score = score(first, *POS_group)\n",
    "            second_score = score(second, 'NOUN,accs')\n",
    "            pos_connects[(first, '{} -> NOUN'.format('|'.join(POS_group)))] = first_score*second_score\n",
    "\n",
    "        # NUMR <- NOUN\n",
    "        if check_POSes('NUMR', 'NOUN'):\n",
    "            pos_connects[(second, 'NUMR <- NOUN')] = score(first, 'NUMR')*score(second, 'NOUN')\n",
    "\n",
    "        # {VERB,ADJF,PRTF,NOUN} -> INFN\n",
    "        if check_POSes('NOUN', 'INFN') and (first[-4:-1] in {'ани', 'ени', 'яни', 'аци', 'тел'} or\n",
    "                                            self.normal_forms(first)&self.verbal_nouns):\n",
    "            pos_connects[(first, 'NOUN -> INFN')] = score(first, 'NOUN')*score(second, 'INFN')\n",
    "        if check_POSes('VERB,ADJF,PRTF', 'INFN'):\n",
    "            POS_group = POSes['first']&{'VERB', 'ADJF', 'PRTF'}\n",
    "            pos_connects[(first, '{} -> INFN'.format('|'.join(POS_group)))] = score(first, *POS_group)*score(second, 'INFN')\n",
    "\n",
    "        # ADVB <- {ADVB,ADJF,PRTF,VERB,INFN}\n",
    "        if check_POSes('ADVB', 'ADVB,ADJF,PRTF,VERB,INFN'):\n",
    "            POS_group = POSes['second']&{'ADVB', 'ADJF', 'PRTF', 'VERB', 'INFN'}\n",
    "            pos_connects[(second, 'ADVB <- {}'.format('|'.join(POS_group)))] = score(first, 'ADVB')*score(second, *POS_group)\n",
    "        \n",
    "        return sorted(pos_connects.items(), key = lambda item: item[1], reverse = True)\n",
    "        \n",
    "parser = BigramParser()\n",
    "print_part = '\\n'.join('{} + {}'.format(item[0], ', '.join(item[1])) for item in list(parser.prep_cases.items())[:20])\n",
    "print('preposition to cases:\\n{}\\n...\\n\\nverbal nouns:\\n{}'.format(print_part, ', '.join(parser.verbal_nouns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Демонстрация работы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* приветливый взор\n",
      "('взор', 'ADJF <- NOUN') : 1.0\n",
      "* открытый взору\n",
      "('открытый', 'PRTF -> NOUN') : 0.5\n",
      "* в город\n",
      "('в', 'PREP -> NOUN') : 0.999763000236\n",
      "* освобождение от\n",
      "('освобождение', 'NOUN -> PREP') : 0.9999990000000001\n",
      "* перевозка грузов\n",
      "('перевозка', 'NOUN -> NOUN') : 1.0\n",
      "* ложку сахару\n",
      "('ложку', 'NOUN -> NOUN') : 0.3333333333333333\n",
      "* перевозит грузы\n",
      "('перевозит', 'VERB -> NOUN') : 0.5\n",
      "* умеет плавать\n",
      "('умеет', 'VERB -> INFN') : 1.0\n",
      "* умение плавать\n",
      "('умение', 'NOUN -> INFN') : 1.0\n",
      "* готовый помочь\n",
      "('готовый', 'ADJF -> INFN') : 0.975903\n",
      "* очень хорошо\n",
      "('хорошо', 'ADVB <- ADVB') : 0.5\n",
      "* весьма интересный\n",
      "('интересный', 'ADVB <- ADJF') : 0.999999\n",
      "* быстро бежит\n",
      "('бежит', 'ADVB <- VERB') : 0.954545\n",
      "* пять машин\n",
      "('машин', 'NUMR <- NOUN') : 0.999999\n"
     ]
    }
   ],
   "source": [
    "bigramms = ['приветливый взор', 'открытый взору', 'в город', 'освобождение от', \n",
    "            'перевозка грузов', 'ложку сахару', 'перевозит грузы', 'умеет плавать', \n",
    "            'умение плавать', 'готовый помочь', 'очень хорошо', 'весьма интересный', \n",
    "            'быстро бежит', 'пять машин']\n",
    "for b in bigramms:\n",
    "    print('* {}\\n{}'.format(b, '\\n'.join(map(lambda item: '{} : {}'.format(*item), parser.connect(*re.split(' ', b))))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Неоднозначные ситуации и вероятности для каждого из вариантов связи:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* готовая помочь\n",
      "('готовая', 'ADJF -> INFN') : 0.975903\n",
      "('помочь', 'ADJF <- NOUN') : 0.012048\n",
      "* три машины\n",
      "('машины', 'NUMR <- NOUN') : 0.857139428574\n",
      "('три', 'VERB -> NOUN') : 0.0037592819550000007\n",
      "* мой какаду\n",
      "('какаду', 'ADJF <- NOUN') : 0.1111111111111111\n",
      "('мой', 'VERB -> NOUN') : 0.05555555555555555\n"
     ]
    }
   ],
   "source": [
    "bigramms = ['готовая помочь', 'три машины', 'мой какаду']\n",
    "for b in bigramms:\n",
    "    print('* {}\\n{}'.format(b, '\\n'.join(map(lambda item: '{} : {}'.format(*item), parser.connect(*re.split(' ', b))))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Работа написанного нами синтаксического анализатора пары слов для различных типов связи:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "количественный тип связи:\n",
      "* пять машин\n",
      "('машин', 'NUMR <- NOUN') : 0.999999\n",
      "\n",
      "отпредложный тип связи:\n",
      "* в здание\n",
      "('в', 'PREP -> NOUN') : 0.9997630002360001\n",
      "* с маслом\n",
      "('с', 'PREP -> NOUN') : 0.998363\n",
      "* на полу\n",
      "('на', 'PREP -> NOUN') : 0.99931\n",
      "\n",
      "определительный тип связи:\n",
      "* очень хорошо\n",
      "('хорошо', 'ADVB <- ADVB') : 0.5\n",
      "* важные вопросы\n",
      "('вопросы', 'ADJF <- NOUN') : 0.9999990000000001\n",
      "* актовому залу\n",
      "('залу', 'ADJF <- NOUN') : 0.6666666666666666\n",
      "* вполне приемлимо\n",
      "('приемлимо', 'ADVB <- ADVB') : 0.05128205128205128\n",
      "\n",
      "посессивный тип связи:\n",
      "* книга врача\n",
      "('книга', 'NOUN -> NOUN') : 0.666666\n",
      "* жертвы теракта\n",
      "('жертвы', 'NOUN -> NOUN') : 1.0\n",
      "\n",
      "предикат и субъект:\n",
      "* спасатели обнаружили\n",
      "\n",
      "\n",
      "прямообъектный тип связи:\n",
      "* уделить внимание\n",
      "('уделить', 'INFN -> NOUN') : 0.875\n",
      "* вижу лес\n",
      "('вижу', 'VERB -> NOUN') : 0.25\n",
      "\n",
      "аппозитивный тип связи:\n",
      "* мальчик Петя\n",
      "\n",
      "\n",
      "обстоятельственный тип связи:\n",
      "* быстро бежать\n",
      "('бежать', 'ADVB <- INFN') : 0.954545\n",
      "* идти медленно\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bigramms = {'прямообъектный тип связи' : ['уделить внимание', 'вижу лес'],\n",
    "            'определительный тип связи' : ['очень хорошо', 'важные вопросы', 'актовому залу', 'вполне приемлимо'],\n",
    "            'отпредложный тип связи' : ['в здание', 'с маслом', 'на полу'],\n",
    "            'предикат и субъект' : ['спасатели обнаружили'],\n",
    "            'посессивный тип связи' : ['книга врача', 'жертвы теракта'],\n",
    "            'аппозитивный тип связи' : ['мальчик Петя'],\n",
    "            'количественный тип связи' : ['пять машин'],\n",
    "            'обстоятельственный тип связи' : ['быстро бежать', 'идти медленно']}\n",
    "for key, value in bigramms.items():\n",
    "    print('{}:'.format(key))\n",
    "    for b in value:\n",
    "        print('* {}\\n{}'.format(b, '\\n'.join(map(lambda item: '{} : {}'.format(*item), parser.connect(*re.split(' ', b))))))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как можно видеть, наиболее распространенные типы связей, кроме аппозитивного и связи предиката и субъекта, распознаются."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Часть 3. Поиск синтаксических связей в предложении."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим класс <b>SentParser</b> (унаследованный от <b>BigramParser</b>), который реализует поиск синтаксических связей в предложении при помощи метода parse, который ставит в соответствие предложению упорядоченную последовательность наиболее вероятных связей вида  \n",
    "<b>[{'nums' : порядковые номера слов данной связи в предложении,   \n",
    "'words' : слова данной связи,  \n",
    "'main_w' : главное слово данной связи,  \n",
    "'label' : тип данной связи,  \n",
    "'score' : вероятность данной связи}]</b>  \n",
    "Последовательность упорядочена по параметру 'nums'; каждой связи таким образом ставится в соответствие ровно один вариант правила - он выбирается на основе параметра 'score' (как наиболее вероятный).\n",
    "\n",
    "У класса <b>SentParser</b> есть два параметра, которые непосредственно влияют на его работу:\n",
    "* параметр <i>min_score</i> показывает, насколько малой может быть вероятность найденного типа связи для включения ее в список; используется в основном методе parse;\n",
    "* параметр <i>distance</i> показывает, насколько далеки могут быть слова, которые проверяются нами на синтаксическую связанность; используется во вспомогательном методе neighbors.\n",
    "\n",
    "Вспомогательный метод neighbors для слова возвращает список \"соседних\" с ним слов: так как наши правила применялись для упорядоченных слов, то и \"соседними\" словами будут являться те, что стоят с правой стороны и являются \"вторыми\" для пары (слово, соседнее с ним слово); как было сказано выше, параметр <i>distance</i> оценивает, насколько далекими могут быть \"соседние слова\".\n",
    "\n",
    "Вспомогательный метод word_split выполняет простейшее разбиение на слова, понимая под ними последовательность букв, цифр, нижнего подчеркивания и дефиса: дефис значим, поскольку используемый нами морфоанализатор умеет анализировать такие слова, как \"из-за\" и \"как-то\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'из-за':\n",
      "из-за + (PREP)\n",
      "'как-то':\n",
      "как-то + (ADVB)\n"
     ]
    }
   ],
   "source": [
    "print(word_parse('из-за'))\n",
    "print(word_parse('как-то'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом, метод parse класса <b>SentParser</b>, инициализированного с параметром distance = 1, для словосочетания 'x y z w' будет проверять на синтаксическую связанность пары (x, y, (y, z) и (z, w); если параметр distance = 2, то будут проверяться пары (x, y), (x, z), (y, z), (y, w), (z, w) - и именно в такой последовательности."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentParser(BigramParser):\n",
    "    def __init__(self, distance = 2, min_score = 1e-2):\n",
    "        # bi_parser initialization\n",
    "        super().__init__()\n",
    "        # word segmentation function\n",
    "        self.word_split = lambda text: re.findall('[\\w-]+', text)\n",
    "        \n",
    "        self.distance = distance\n",
    "        self.neighbors = lambda sent, num: list(enumerate(sent))[num + 1 : num + self.distance + 1]\n",
    "        \n",
    "        self.min_score = min_score\n",
    "    \n",
    "    def parse(self, sentence):\n",
    "        sent = self.word_split(sentence)\n",
    "        \n",
    "        sent_parse = [(((i, j), (w_i, w_j)), self.connect(w_i, w_j))\n",
    "                      for i, w_i in enumerate(sent) for j, w_j in self.neighbors(sent, i)]\n",
    "        sent_parse = [(pair, parse[0]) for pair, parse in sent_parse if parse and parse[0][1] >= self.min_score]\n",
    "        connect_dict = lambda b: {'nums' : b[0][0], 'words' : b[0][1],\n",
    "                                  'main_w' : b[1][0][0], 'label' : b[1][0][1],\n",
    "                                  'score' : b[1][1]}\n",
    "        return [connect_dict(connect) for connect in sent_parse]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Демонстрация работы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* исключительно интересный фильм\n",
      "words : ('исключительно', 'интересный'), score : 0.24999975, nums : (0, 1), main_w : интересный, label : ADVB <- ADJF\n",
      "words : ('интересный', 'фильм'), score : 0.999999, nums : (1, 2), main_w : фильм, label : ADJF <- NOUN\n",
      "* красный полосатый мяч\n",
      "words : ('полосатый', 'мяч'), score : 0.999999, nums : (1, 2), main_w : мяч, label : ADJF <- NOUN\n",
      "* любит весело играть\n",
      "words : ('весело', 'играть'), score : 0.857142, nums : (1, 2), main_w : играть, label : ADVB <- INFN\n",
      "* при большом желании\n",
      "words : ('большом', 'желании'), score : 0.272727, nums : (1, 2), main_w : желании, label : ADJF <- NOUN\n"
     ]
    }
   ],
   "source": [
    "parser = SentParser(1)\n",
    "trigramms = ['исключительно интересный фильм', 'красный полосатый мяч',\n",
    "             'любит весело играть', 'при большом желании']\n",
    "str_connect = lambda c: ', '.join(map(lambda i: '{} : {}'.format(*i), sorted(c.items(), reverse = True)))\n",
    "for t in trigramms:\n",
    "    print('* {}\\n{}'.format(t, '\\n'.join(map(str_connect, parser.parse(t)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* исключительно интересный фильм\n",
      "words : ('исключительно', 'интересный'), score : 0.24999975, nums : (0, 1), main_w : интересный, label : ADVB <- ADJF\n",
      "words : ('интересный', 'фильм'), score : 0.999999, nums : (1, 2), main_w : фильм, label : ADJF <- NOUN\n",
      "* красный полосатый мяч\n",
      "words : ('красный', 'мяч'), score : 0.666666, nums : (0, 2), main_w : мяч, label : ADJF <- NOUN\n",
      "words : ('полосатый', 'мяч'), score : 0.999999, nums : (1, 2), main_w : мяч, label : ADJF <- NOUN\n",
      "* любит весело играть\n",
      "words : ('любит', 'играть'), score : 1.0, nums : (0, 2), main_w : любит, label : VERB -> INFN\n",
      "words : ('весело', 'играть'), score : 0.857142, nums : (1, 2), main_w : играть, label : ADVB <- INFN\n",
      "* при большом желании\n",
      "words : ('при', 'желании'), score : 0.99931, nums : (0, 2), main_w : при, label : PREP -> NOUN\n",
      "words : ('большом', 'желании'), score : 0.272727, nums : (1, 2), main_w : желании, label : ADJF <- NOUN\n"
     ]
    }
   ],
   "source": [
    "parser = SentParser(2)\n",
    "for t in trigramms:\n",
    "    print('* {}\\n{}'.format(t, '\\n'.join(map(str_connect, parser.parse(t)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как можно видеть, равенство параметра distance двум позволяет находить все возможные синтаксические связи в триграммах: при distance = 1 обрабатываются только соседние слова и не учитываются более сложные, комбинированные типы связей (кроме последовательных, как в случае с \"исключительно интересным фильмом\").\n",
    "\n",
    "###### Тем не менее, большое значение параметра distance может приводить к ошибочно найденным связям:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* очень быстро бежать\n",
      "words : ('очень', 'быстро'), score : 0.954545, nums : (0, 1), main_w : быстро, label : ADVB <- ADVB\n",
      "words : ('очень', 'бежать'), score : 1.0, nums : (0, 2), main_w : бежать, label : ADVB <- INFN\n",
      "words : ('быстро', 'бежать'), score : 0.954545, nums : (1, 2), main_w : бежать, label : ADVB <- INFN\n",
      "* связь морфологии и синтаксиса\n",
      "words : ('связь', 'морфологии'), score : 0.333333, nums : (0, 1), main_w : связь, label : NOUN -> NOUN\n",
      "words : ('связь', 'синтаксиса'), score : 1.0, nums : (0, 3), main_w : связь, label : NOUN -> NOUN\n",
      "words : ('морфологии', 'синтаксиса'), score : 0.999997, nums : (1, 3), main_w : морфологии, label : NOUN -> NOUN\n"
     ]
    }
   ],
   "source": [
    "parser = SentParser(3)\n",
    "trigramms = ['очень быстро бежать', 'связь морфологии и синтаксиса']\n",
    "for t in trigramms:\n",
    "    print('* {}\\n{}'.format(t, '\\n'.join(map(str_connect, parser.parse(t)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратим отдельное внимание на граф зависимостей обрабатываемых словосочетаний: в случае distance = 1 словосочетание разбивается на некоторое количество деревьев: используемые нами правила не предполагают ситуации, в которой одно из слов становится зависимым от слова слева от него и слова справа.\n",
    "\n",
    "##### Но уже при distance = 2 возможно появление циклов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* начавшееся днем кино\n",
      "words : ('начавшееся', 'днем'), score : 0.5, nums : (0, 1), main_w : начавшееся, label : PRTF -> NOUN\n",
      "words : ('начавшееся', 'кино'), score : 0.333333, nums : (0, 2), main_w : кино, label : PRTF <- NOUN\n",
      "words : ('днем', 'кино'), score : 0.0714285, nums : (1, 2), main_w : днем, label : NOUN -> NOUN\n"
     ]
    }
   ],
   "source": [
    "parser = SentParser(2)\n",
    "t = 'начавшееся днем кино'\n",
    "print('* {}\\n{}'.format(t, '\\n'.join(map(str_connect, parser.parse(t)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* начавшееся -> днем, днем -> кино, кино -> начавшееся  \n",
    "появление цикла по причине реализации правила NOUN -> NOUN (+ gen, gen_2)\n",
    "\n",
    "Лишнюю связь можно было бы отбросить, но для этого нужно работать с построенным графом целиком и строить правила для согласованности правил синтаксических связей, а также оцениванивать \"вероятность\" построенных деревьев: действительно, проблема данного словосочетания и в его омонимичности, и лишней будет другая связь:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* мероприятие, начавшееся \"Днем Кино\"\n",
      "words : ('начавшееся', 'Днем'), score : 0.5, nums : (1, 2), main_w : начавшееся, label : PRTF -> NOUN\n",
      "words : ('начавшееся', 'Кино'), score : 0.333333, nums : (1, 3), main_w : Кино, label : PRTF <- NOUN\n",
      "words : ('Днем', 'Кино'), score : 0.0714285, nums : (2, 3), main_w : Днем, label : NOUN -> NOUN\n"
     ]
    }
   ],
   "source": [
    "t = 'мероприятие, начавшееся \"Днем Кино\"'\n",
    "print('* {}\\n{}'.format(t, '\\n'.join(map(str_connect, parser.parse(t)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Этот пример частично освещает разные стороны проблемы построения деревьев зависимости, следования правилам и проч. В данном задании они не будут решаться, потому что в большинстве случаев (как будет видно далее) мы будем получать удовлетворительный результат; тем не менее, будем помнить о возможности получения цикла при distance > 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Часть 4. Синтаксический анализ текста (частичный парсер)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проанализируем работу нашего класса <b>SentParser</b> для нескольких предложений первого текста, сегментированного классом <b>Split</b>, продемонстрировав работу синтаксического анализатора не на словосочетаниях, а на реальных предложениях:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Я  самым внимательным образом изучил карты города, но так и  не отыскал на  них  улицу  д'Осейль\n",
      "words : ('самым', 'образом'), score : 0.148148, nums : (1, 3), main_w : образом, label : ADJF <- NOUN\n",
      "words : ('внимательным', 'образом'), score : 0.3333333333333333, nums : (2, 3), main_w : образом, label : ADJF <- NOUN\n",
      "words : ('образом', 'карты'), score : 0.90909, nums : (3, 5), main_w : образом, label : NOUN -> NOUN\n",
      "words : ('изучил', 'карты'), score : 0.045454, nums : (4, 5), main_w : изучил, label : VERB -> NOUN\n",
      "words : ('карты', 'города'), score : 0.9899470201019999, nums : (5, 6), main_w : карты, label : NOUN -> NOUN\n",
      "words : ('на', 'улицу'), score : 0.99931, nums : (12, 14), main_w : на, label : PREP -> NOUN\n",
      "* Надо сказать, что  я  рылся отнюдь  не  только  в современных  картах,  поскольку мне  было  известно, что  подобные  названия нередко меняются\n",
      "words : ('что', 'рылся'), score : 0.014925, nums : (2, 4), main_w : рылся, label : ADVB <- VERB\n",
      "words : ('в', 'картах'), score : 0.999764, nums : (8, 10), main_w : в, label : PREP -> NOUN\n",
      "words : ('современных', 'картах'), score : 0.210526, nums : (9, 10), main_w : картах, label : ADJF <- NOUN\n",
      "words : ('что', 'подобные'), score : 0.014924985075000001, nums : (15, 16), main_w : подобные, label : ADVB <- ADJF\n",
      "words : ('подобные', 'названия'), score : 0.054053945946, nums : (16, 17), main_w : названия, label : ADJF <- NOUN\n",
      "words : ('нередко', 'меняются'), score : 0.875, nums : (18, 19), main_w : меняются, label : ADVB <- VERB\n"
     ]
    }
   ],
   "source": [
    "for i, sent_i in split[(0,1)][:2]:\n",
    "    print('* {}\\n{}'.format(sent_i.replace('\\n', ' '), '\\n'.join(map(str_connect, parser.parse(sent_i)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Словосочетание \"отыскал на них\" вообще не обработалось! Кроме того, не была выделена связь \"самым внимательным\" - поскольку \"самый\" является местоименным прилагательным, но не наречием; невыделение прочих связей, таких как \"образом изучил\" (обратный порядок слов - не рассматриваем), \"улицу д'Осейль\" (аппозитивный тип связи - не рассматриваем) обсуждалось ранее.\n",
    "\n",
    "Ранее мы уже говорили об особенностях разбора местоимений, - с одной стороны, зачастую в их разборах присутствует отличная от NPRO часть речи, что позволяет нам применить имеющиеся правила; с другой стороны, в их разборах никогда не присутствует существительных. Мы не будем переписывать все правила таким образом, чтобы они работали и для местоимений, заменяющих существительные; вместо этого мы добавим правило связи предлога и местоимения по аналогии со связью Prep -> N: в нем будет проверяться, есть ли вообще падеж у предлога + проверим, что это местоимение не может играть роль прилагательного или числительного, то есть во всех разборах оно обладает тегом 'NPRO'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'NPRO'}\n",
      "{'NPRO'}\n"
     ]
    }
   ],
   "source": [
    "print(parser.morph_tags('них', 'POS'))\n",
    "print(parser.morph_tags('кого-то', 'POS'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поскольку в наших правилах есть 'N -> Prep', где Ν - отглагольное существительное, то было бы логичным добавить связь <b>'V -> Prep'</b> (будем помнить о том, что при distance > 1 в словосочетании \"получить освобождение от работ\" будет найдена лишняя связь; впрочем, и с имеющимся набором правил в словосочетании \"поедание хлеба с маслом\" будет найдена лишняя связь - более того, неясно, \"с маслом\" - это определение хлеба или определение поедания: все не так просто)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* поедание хлеба с маслом\n",
      "words : ('поедание', 'хлеба'), score : 0.923076, nums : (0, 1), main_w : поедание, label : NOUN -> NOUN\n",
      "words : ('поедание', 'с'), score : 0.998363, nums : (0, 2), main_w : поедание, label : NOUN -> PREP\n",
      "words : ('с', 'маслом'), score : 0.998363, nums : (2, 3), main_w : с, label : PREP -> NOUN\n"
     ]
    }
   ],
   "source": [
    "t = 'поедание хлеба с маслом'\n",
    "print('* {}\\n{}'.format(t, '\\n'.join(map(str_connect, parser.parse(t)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Parser\n",
    "Итак, напишем парсер нашего текста. Он реализует свойства как класса <b>Split</b>, так и класса <b>SentParser</b>, - унаследуем от них создаваемый класс <b>Parser</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Модификации методов родительских классов и методы, их использующие\n",
    "\n",
    "* Используя метод word_split класса <b>SentParser</b>, добавим метод сегментации на слова: word_segment.\n",
    "* Дополним метод connect класса <b>BigramParser</b>, унаследованный классом <b>SentParser</b>, двумя правилами: V -> Prep и Prep -> Npro.\n",
    "* Модифицируем метод parse класса <b>SentParser</b>, написав новый метод sent_parse: по номеру предложения в структуре, полученной методами класса <b>Split</b>, он возвращает список вида <b>[((индекс главного слова, индекс зависимого слова), тип связи)]</b>, где индексирование соответствует индексам слов в структуре, полученной применением метода word_segment.\n",
    "* О прочих методах и членах класса <b>Parser</b> будет сказано позже."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Члены класса Parser\n",
    "\n",
    "Помимо членов, определяемых классами <b>Split</b> и <b>SentParser</b>, определим следующие члены:\n",
    "* connects - словарь найденных синтаксических связей: он будет инициализироваться, как <b>{(индекс главного слова, индекс зависимого слова) : тип связи}</b> путем применения метода sent_parse для всех предложений структуры.\n",
    "* graph - граф найденных синтаксических связей, представленный списком смежности зависимых от него слов с информацией о типе связи, то есть словарь вида <b>{индекс главного слова : [{зависимое слово : тип зависимости}]</b>.\n",
    "\n",
    "Инициализация graph производится методом graph_construct на основе полученного словаря connects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Parser(Split, SentParser):\n",
    "    def __init__(self, *names, distance = 1, min_score = 1e-2):\n",
    "        Split.__init__(self, *names)\n",
    "        SentParser.__init__(self, distance, min_score)\n",
    "        # word segmentation\n",
    "        self.word_segment = lambda: self.segment(self.texts, self.word_split)\n",
    "        # dictionary of syntax connections\n",
    "        self.connects = {}\n",
    "        # syntax graph\n",
    "        self.graph = {}\n",
    "    def connect(self, first, second):\n",
    "        pos_connects = dict(SentParser.connect(self, first, second))\n",
    "        \n",
    "        POSes = dict(zip(('first', 'second'), map(lambda word: set(self.morph_tags(word, 'POS')), (first, second))))\n",
    "        check_POSes = lambda p_1, p_2: set(p_1.split(','))&POSes['first'] and set(p_2.split(','))&POSes['second']\n",
    "        score = lambda word, *pos_tags: sum(score for tag, score in self.tag_scores(word).items() for t in pos_tags\n",
    "                                            if 'None' not in t and set(t.split(',')) in tag)\n",
    "        \n",
    "        # {VERB,INFN} -> PREP\n",
    "        if check_POSes('VERB,INFN', 'PREP'):\n",
    "            POS_group = POSes['first']&{'VERB', 'INFN'}\n",
    "            pos_connects[(first, '{} -> PREP'.format('|'.join(POS_group)))] = score(first, *POS_group)*score(second, 'PREP')\n",
    "        \n",
    "        # PREP -> NPRO (~ PREP -> NOUN)\n",
    "        if check_POSes('PREP', 'NPRO') and score(second, 'NPRO') > 1.0 - self.min_score:\n",
    "            if first.lower() in self.prep_cases and self.prep_cases[first.lower()]&self.morph_tags(second, 'case'):\n",
    "                pos_connects[(first, 'PREP -> NPRO')] = score(first, 'PREP')*score(second, 'NPRO')\n",
    "        \n",
    "        return sorted(pos_connects.items(), key = lambda item: item[1], reverse = True)\n",
    "    def sent_parse(self, i):\n",
    "        sent_parse = self.parse(self.texts[i])\n",
    "        reverse = lambda nums: tuple(i for i in reversed(nums))\n",
    "        sent_parse = [(reverse(b['nums']) if b['words'].index(b['main_w']) else b['nums'], b['label']) for b in sent_parse]\n",
    "        return [((i + (b[0][0],), i + (b[0][1],)), b[1]) for b in sent_parse]\n",
    "    def text_parse(self):\n",
    "        self.par_segment()\n",
    "        self.sent_segment()\n",
    "        self.connects = {connect : label for i, sent_i in self for connect, label in self.sent_parse(i)}\n",
    "        self.sents = self.texts.copy()\n",
    "        self.word_segment()\n",
    "        self.words = self.texts\n",
    "        self.graph_construct()\n",
    "        return (sorted(self.sents.items()), sorted(self.words.items()), \n",
    "                sorted(self.connects.items()), sorted(self.graph.items()))\n",
    "    def graph_construct(self):\n",
    "        for pair, label in self.connects.items():\n",
    "            if pair[0] not in self.graph:\n",
    "                self.graph[pair[0]] = [{pair[1] : label}]\n",
    "            else:\n",
    "                self.graph[pair[0]].append({pair[1] : label})\n",
    "        return self.graph\n",
    "    def graph_repr(self):\n",
    "        graph_words = {}\n",
    "        for main_w, connects in self.graph.items():\n",
    "            nums = sorted((main_w, *map(lambda w_l: list(w_l)[0], connects)))\n",
    "            num_labels = sorted(map(lambda w_l: list(w_l.items())[0], connects), key = lambda w_l: w_l[0])\n",
    "            graph_words[main_w] = (tuple(map(lambda w: parser[w], nums)),\n",
    "                                   tuple(map(lambda w_l: w_l[1], num_labels)), \n",
    "                                   self.sents[main_w[:-1]])\n",
    "        return graph_words\n",
    "\n",
    "    def tree_repr(self, main_w, depth, words, labels):\n",
    "        if main_w in self.graph and main_w not in words:\n",
    "            words.append(main_w)\n",
    "            for w_l in self.graph[main_w]:\n",
    "                word, label = list(w_l.items())[0]\n",
    "                connect = ' '.join(map(lambda w: self[w], sorted([main_w, word])))\n",
    "                labels.append((depth, connect, label))\n",
    "                self.tree_repr(word, depth + 1, words, labels)\n",
    "        else:\n",
    "            words.append(main_w)\n",
    "        return tuple(map(lambda w: self[w], sorted(list(set(words))))), labels, self.sents[main_w[:-1]]\n",
    "    def forest_repr(self):\n",
    "        return {main_w : self.tree_repr(main_w, 0, [], []) for main_w in self.graph}\n",
    "    \n",
    "    def connect_search(self, label):\n",
    "        l_values = {'concord' : ' <- NOUN', 'AGR' : ' <- NOUN', '<-N' : ' <- NOUN',\n",
    "                    '->noun' : ' -> NOUN', '->N' : ' -> NOUN',\n",
    "                    '->infinitive' : ' -> INFN', '->Inf' : ' -> INFN',\n",
    "                    'infinitive' : 'INFN', 'Inf' : 'INFN',\n",
    "                    'adverb<-' : 'ADVB <- ', 'Adv<-' : 'ADVB <- ',\n",
    "                    'adverb' : 'ADVB', 'Adv' : 'ADVB <- ',\n",
    "                    'preposition' : 'PREP', 'Prep' : 'PREP', \n",
    "                    '->Prep' : ' -> PREP', 'Prep->' : 'PREP -> '}\n",
    "        order = lambda key: (self[key[0]], self[key[1]]) if key[0] < key[1] else (self[key[1]], self[key[0]])\n",
    "        return [(*order(key), value) for key, value in self.connects.items() \n",
    "                if value == label or (label in l_values and l_values[label] in value)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Работа модифицированного метода connect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* бегать по лесу\n",
      "words : ('бегать', 'по'), score : 1.0, nums : (0, 1), main_w : бегать, label : INFN -> PREP\n",
      "words : ('по', 'лесу'), score : 1.0, nums : (1, 2), main_w : по, label : PREP -> NOUN\n",
      "* ставка на них\n",
      "words : ('ставка', 'на'), score : 0.99931, nums : (0, 1), main_w : ставка, label : NOUN -> PREP\n",
      "words : ('на', 'них'), score : 0.99930900069, nums : (1, 2), main_w : на, label : PREP -> NPRO\n"
     ]
    }
   ],
   "source": [
    "parser = Parser()\n",
    "trigramms = ['бегать по лесу', 'ставка на них']\n",
    "for t in trigramms:\n",
    "    print('* {}\\n{}'.format(t, '\\n'.join(map(str_connect, parser.parse(t)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Демонстрация работы.\n",
    "Расскажем про реализованные методы класса <b>Parser</b> и покажем их работу на примере рассказа Говарда Лавкрафта \"Музыка Эриха Цанна\" (http://lib.ru/INOFANT/LAWKRAFT/muz.txt). Для начала будем рассматривать работу класса с distance = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Метод text_parse\n",
    "\n",
    "Данный метод реализует создание структуры уровня предложений и уровня слов на основе текстов, сопоставляя каждому предложению индекс (x, y, z), а слову - индекс (x, y, z, w), где x - номер текста, y - номер абзаца, z - номер предложения, w - номер слова; также создает граф зависимостей на основе найденных в предложениях синтаксических связей.\n",
    "\n",
    "Его работа происходит в несколько этапов:\n",
    "* применение метода par_segment - сегментация текстов на абзацы;\n",
    "* применение метода sent_segment - сегментация полученной структуры на предложения;\n",
    "* инициализация словаря connects путем применения к каждому предложению полученной структуры метода sent_parse;\n",
    "* создание копии структуры, соответствующей уровню предложений: член sents;\n",
    "* применение метода word_segment - сегментация полученной структуры на слова: получение члена words;\n",
    "* инициализация графа зависимостей graph при помощи метода graph_construct.\n",
    "\n",
    "Метод возвращает упорядоченные по индексам инициализированные этим методом члены sents, words, connect и graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0) : Говард Ф\n",
      "(0, 1) : Лавкрафт\n",
      "(0, 2) : Музыка Эриха Цанна\n",
      "(1, 0) : Я  самым внимательным образом изучил карты города, но так и  не отыскал\n",
      "на  них  улицу  д'Осейль\n",
      "(1, 1) : Надо сказать, что  я  рылся отнюдь  не  только  в\n",
      "современных  картах,  поскольку мне  было  известно, что  подобные  названия\n",
      "нередко меняются\n",
      "(1, 2) : Напротив, я, можно сказать,  по уши залез  в седую старину\n",
      "и, более того, лично обследовал интересовавший меня район, уже  не  особенно\n",
      "обращая  внимания на  таблички  и  вывески,  в  поисках  того,  что  хотя бы\n",
      "отдаленно походило на интересовавшую  меня улицу д'Осейль\n",
      "(1, 3) : Однако,  несмотря\n",
      "на  все мои усилия, вынужден сейчас не без  стыда признаться,  что  так и не\n",
      "смог отыскать нужные мне дом, улицу, и даже приблизительно определить район,\n",
      "где,  в  течение  последних месяцев  моей  обездоленной  жизни,  я,  студент\n",
      "факультета метафизики, слушал музыку Эриха Занна\n",
      "(2, 0) : Меня отнюдь не  удивляет  подобный провал в памяти, поскольку за период\n",
      "жизни на улице д'Осейль я серьезно подорвал как физическое, так и умственное\n",
      "здоровье,  и  потому  был   не  в  состоянии  вспомнить  ни  одного  из  тех\n",
      "немногочисленных знакомых, которые у меня там появились, Однако то, что я не\n",
      "могу припомнить  само это  место, кажется  мне  не  просто  странным,  но  и\n",
      "поистине  обескураживающим,  поскольку  располагалось  оно  не  далее, чем в\n",
      "получасе ходьбы от  университета, и было отмечено рядом весьма специфических\n",
      "особенностей, которые  едва  ли стерлись  бы  в  памяти любого, кто  хотя бы\n",
      "однажды там побывал\n",
      "(3, 0) : И все  же  мне  ни разу не  довелось повстречать  человека, который  бы\n",
      "слышал про улицу д'Осейль\n",
      "(4, 0) : По массивному, сложенному из  черного камня мосту  улица эта пересекала\n",
      "темную реку, вдоль которой располагались кирпичные стены складских помещений\n",
      "с  помутневшими  окнами\n",
      "....................................................................................................\n",
      "(0, 0, 0) : Говард\n",
      "(0, 0, 1) : Ф\n",
      "(0, 1, 0) : Лавкрафт\n",
      "(0, 2, 0) : Музыка\n",
      "(0, 2, 1) : Эриха\n",
      "(0, 2, 2) : Цанна\n",
      "(1, 0, 0) : Я\n",
      "(1, 0, 1) : самым\n",
      "(1, 0, 2) : внимательным\n",
      "(1, 0, 3) : образом\n",
      "....................................................................................................\n",
      "((0, 0, 0), (0, 0, 1)) : NOUN -> NOUN\n",
      "((0, 2, 0), (0, 2, 1)) : NOUN -> NOUN\n",
      "((1, 0, 3), (1, 0, 2)) : ADJF <- NOUN\n",
      "((1, 0, 4), (1, 0, 5)) : VERB -> NOUN\n",
      "((1, 0, 5), (1, 0, 6)) : NOUN -> NOUN\n",
      "((1, 0, 11), (1, 0, 12)) : VERB -> PREP\n",
      "((1, 0, 12), (1, 0, 13)) : PREP -> NPRO\n",
      "((1, 1, 10), (1, 1, 9)) : ADJF <- NOUN\n",
      "((1, 1, 16), (1, 1, 15)) : ADVB <- ADJF\n",
      "((1, 1, 17), (1, 1, 16)) : ADJF <- NOUN\n",
      "....................................................................................................\n",
      "(0, 0, 0) : [{(0, 0, 1): 'NOUN -> NOUN'}]\n",
      "(0, 2, 0) : [{(0, 2, 1): 'NOUN -> NOUN'}]\n",
      "(1, 0, 3) : [{(1, 0, 2): 'ADJF <- NOUN'}]\n",
      "(1, 0, 4) : [{(1, 0, 5): 'VERB -> NOUN'}]\n",
      "(1, 0, 5) : [{(1, 0, 6): 'NOUN -> NOUN'}]\n",
      "(1, 0, 11) : [{(1, 0, 12): 'VERB -> PREP'}]\n",
      "(1, 0, 12) : [{(1, 0, 13): 'PREP -> NPRO'}]\n",
      "(1, 1, 10) : [{(1, 1, 9): 'ADJF <- NOUN'}]\n",
      "(1, 1, 16) : [{(1, 1, 15): 'ADVB <- ADJF'}]\n",
      "(1, 1, 17) : [{(1, 1, 16): 'ADJF <- NOUN'}]\n"
     ]
    }
   ],
   "source": [
    "parser = Parser('text_1.txt', distance = 1)\n",
    "print('\\n{}\\n'.format('.'*100).join(map(lambda s: '\\n'.join(map(lambda c: '{} : {}'.format(*c), s[:10])), parser.text_parse())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Метод connect_search\n",
    "\n",
    "Данный метод возвращает список пар слов (в их изначальном порядке), которым методом text_parse был сопоставлен подаваемый на вход тип связи.\n",
    "\n",
    "Помимо обработки прямого совпадения типа связи реализован пользовательский словарь:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* connections with concord:\n",
      "Дорожное покрытие : ADJF <- NOUN\n",
      "бешеных мотивах : ADJF <- NOUN\n",
      "необитаемого помещения : ADJF <- NOUN\n",
      "его голове : ADJF <- NOUN\n",
      "мой плащ : ADJF <- NOUN\n",
      "потертой одежде : PRTF|ADJF <- NOUN\n",
      "колдовские мотивы : ADJF <- NOUN\n",
      "этой мансарде : ADJF <- NOUN\n",
      "странный тип : ADJF <- NOUN\n",
      "их источника : ADJF <- NOUN\n",
      "\n",
      "* connections with PRTF <- NOUN:\n",
      "обездоленной жизни : PRTF <- NOUN\n",
      "запомнившихся мелодий : PRTF <- NOUN\n",
      "зашторенного окна : PRTF <- NOUN\n",
      "скрюченный мужчина : PRTF <- NOUN\n",
      "покосившимися домами : PRTF <- NOUN\n",
      "запертой двери : PRTF <- NOUN\n",
      "захламленному столу : PRTF <- NOUN\n",
      "одуряющими звуками : PRTF <- NOUN\n",
      "зашторенного окна : PRTF <- NOUN\n",
      "разбросанные груды : PRTF <- NOUN\n",
      "\n",
      "* connections with ->noun:\n",
      "булыжником улицы : NOUN -> NOUN\n",
      "вылетел конец : VERB -> NOUN\n",
      "в получасе : PREP -> NOUN\n",
      "на полу : PREP -> NOUN\n",
      "под дверью : PREP -> NOUN\n",
      "игру Эриха : NOUN -> NOUN\n",
      "на земле : PREP -> NOUN\n",
      "к столу : PREP -> NOUN\n",
      "юбку матери : NOUN -> NOUN\n",
      "скрипнул ставень : VERB -> NOUN\n",
      "\n",
      "* connections with Prep->:\n",
      "с ним : PREP -> NPRO\n",
      "в получасе : PREP -> NOUN\n",
      "на полу : PREP -> NOUN\n",
      "под дверью : PREP -> NOUN\n",
      "на земле : PREP -> NOUN\n",
      "к столу : PREP -> NOUN\n",
      "по ночам : PREP -> NOUN\n",
      "над головой : PREP -> NOUN\n",
      "в комнате : PREP -> NOUN\n",
      "с запада : PREP -> NOUN\n",
      "\n",
      "* connections with Inf:\n",
      "показаться в : INFN -> PREP\n",
      "желание выглянуть : NOUN -> INFN\n",
      "повторно найти : ADVB <- INFN\n",
      "повстречать человека : INFN -> NOUN\n",
      "отреагировать на : INFN -> PREP\n",
      "могу припомнить : VERB -> INFN\n",
      "довелось повстречать : VERB -> INFN\n",
      "было отдернуть : VERB -> INFN\n",
      "пытался подняться : VERB -> INFN\n",
      "вновь зажечь : ADVB <- INFN\n",
      "\n",
      "* connections with Adv<-:\n",
      "вдруг заметил : ADVB <- VERB\n",
      "действительно искренне : ADVB <- ADVB\n",
      "все это : ADVB <- ADJF\n",
      "неумело насвистывал : ADVB <- VERB\n",
      "кверху чердачном : ADVB <- ADJF\n",
      "навечно поселились : ADVB <- VERB\n",
      "дома чуть : ADVB <- ADVB\n",
      "Более того : ADVB <- ADJF\n",
      "постоянно пребывали : ADVB <- VERB\n",
      "как получилось : ADVB <- VERB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for bond in ['concord', 'PRTF <- NOUN', '->noun', 'Prep->', 'Inf', 'Adv<-']:\n",
    "    print('* connections with {}:\\n{}\\n'.format(bond, '\\n'.join(map(lambda c: '{} {} : {}'.format(*c),\n",
    "                                               parser.connect_search(bond)[:10]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можем видеть, что правила синтаксических связей достаточно неплохо работают!  \n",
    "По крайней мере, вырванные из контекста, данные словосочетания выглядят абсолютно нормально."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Методы представления graph\n",
    "\n",
    "Нами было реализовано два метода представления полученного списка смежностей:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### graph_repr: \n",
    "\n",
    "* возвращает словарь вида <b>{главное слово : (упорядоченный список из зависимых слов и главного, список типов связей, предложение с этими словами)]}</b></b>, где упорядочивание производится согласно индексам слов, то есть в том порядке, в котором они изначально были в предложении;  \n",
    "* так, словосочетание \"срочная перевозка грузов\" при distance = 1 будет обработано следующим образом:\n",
    "перевозка : срочная перевозка грузов : A <- N, N -> N\n",
    "\n",
    "Покажем работу этого метода, отсортировав полученный словарь по длине списка слов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "покрытие : Дорожное покрытие улицы : ADJF <- NOUN, NOUN -> NOUN\n",
      "места : этого места сосредоточения : ADJF <- NOUN, NOUN -> NOUN\n",
      "ушел : совсем ушел в : ADVB <- VERB, VERB -> PREP\n",
      "светом : лунным светом крыш : ADJF <- NOUN, NOUN -> NOUN\n",
      "бездне : невообразимой бездне мрака : ADJF <- NOUN, NOUN -> NOUN\n",
      "желание : давнее желание выглянуть : ADJF <- NOUN, NOUN -> INFN\n",
      "крыши : поблескивающие крыши домов : PRTF <- NOUN, NOUN -> NOUN\n",
      "желание : своенравное желание выглянуть : ADJF <- NOUN, NOUN -> INFN\n",
      "время : любое время суток : ADJF <- NOUN, NOUN -> NOUN\n",
      "помигивали : приветливо помигивали огни : ADVB <- VERB, VERB -> NOUN\n",
      "порыв : налетевший порыв ветра : PRTF <- NOUN, NOUN -> NOUN\n",
      "превышавшей : намного превышавшей уровень : ADVB <- PRTF, PRTF -> NOUN\n",
      "расположение : дружеское расположение в : ADJF <- NOUN, NOUN -> PREP\n",
      "захлопали : оглушительно захлопали по : ADVB <- VERB, VERB -> PREP\n",
      "обилие : повсеместное обилие пыли : ADJF <- NOUN, NOUN -> NOUN\n",
      "вздрогнул : невольно вздрогнул от : ADVB <- VERB, VERB -> PREP\n",
      "находилось : что находилось в : ADVB <- VERB, VERB -> PREP\n",
      "набрался : вовсе набрался смелости : ADVB <- VERB, VERB -> NOUN\n",
      "инструмента : его инструмента тем : ADJF <- NOUN, NOUN -> NOUN\n",
      "пришел : вскоре пришел к : ADVB <- VERB, VERB -> PREP\n"
     ]
    }
   ],
   "source": [
    "graph_repr = parser.graph_repr()\n",
    "for main_w, bond_words in list(sorted(list(graph_repr.items()),\n",
    "                                      key = lambda item: len(item[1][0]),\n",
    "                                      reverse = True))[:20]:\n",
    "    print('{} : {} : {}'.format(parser[main_w], ' '.join(bond_words[0]), ', '.join(bond_words[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поскольку distance = 1, то длина полученных наиболее длинных словосочетаний не превышает 3: поскольку список смежности для каждого из слов состоит не более чем из двух зависимых от него.\n",
    "\n",
    "Тем не менее, это первый шаг к выделению словосочетаний, полученных применением нескольких правил."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### forest_repr:\n",
    "\n",
    "* вспомогательный метод <i>tree_repr</i>:  \n",
    "    * для одного из \"главных\" слов рекурсивно получает все индексы слов, до которых можно добраться, следуя стрелкам найденных синтаксических связей, а также список триплетов (глубина рекурсии, словосочетание, тип связи); затем упорядочивает индексы и сопоставляет им сами слова, полученный же список пар остается в порядке действия рекурсии; возвращает отсортированный список слов и список триплетов (глубина рекурсии, словосочетание, тип связи) в рекурсивном порядке;  \n",
    "    * так, словосочетание \"срочная перевозка грузов до города\" для слова \"перевозка\" при distance = 1 будет обработано следующим образом:  \n",
    "срочная перевозка грузов до города:\n",
    "...срочная перевозка : Α <- N\n",
    "перевозка грузов : N -> N\n",
    "грузов до : N -> Prep\n",
    "до города : Prep -> N  \n",
    "* <b>forest_repr</b>:\n",
    "    * возвращает словарь вида <b>{главное слово : дерево, полученное применением метода tree_repr}</b>\n",
    "    * стоит отметить, что многие из полученных последовательностей слов будут вложенными друг в друга, так, словосочетание \"срочная перевозка грузов до города\" для слова \"грузов\" дает дерево \"грузов до города\" $\\subset$ \"срочная перевозка грузов до города\"\n",
    "    \n",
    "Покажем работу этого метода, отсортировав полученный словарь по длине списка слов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "после чего снова опустился на стул:\n",
      "    опустился на : VERB -> PREP\n",
      "    ...на стул : PREP -> NOUN\n",
      "    снова опустился : ADVB <- VERB\n",
      "    ...чего снова : ADVB <- ADVB\n",
      "    ......после чего : ADVB <- ADVB\n",
      "\n",
      "буквально светилось от облегчения при виде:\n",
      "    светилось от : VERB -> PREP\n",
      "    ...от облегчения : PREP -> NOUN\n",
      "    ......облегчения при : NOUN -> PREP\n",
      "    .........при виде : PREP -> NOUN\n",
      "    буквально светилось : ADVB <- VERB\n",
      "\n",
      "этого места сосредоточения ночных кошмаров:\n",
      "    этого места : ADJF <- NOUN\n",
      "    места сосредоточения : NOUN -> NOUN\n",
      "    ...сосредоточения ночных : NOUN -> NOUN\n",
      "    ......ночных кошмаров : NOUN -> NOUN\n",
      "\n",
      "давнее желание выглянуть из того:\n",
      "    желание выглянуть : NOUN -> INFN\n",
      "    ...выглянуть из : INFN -> PREP\n",
      "    ......из того : PREP -> NOUN\n",
      "    давнее желание : ADJF <- NOUN\n",
      "\n",
      "в получасе ходьбы от университета:\n",
      "    в получасе : PREP -> NOUN\n",
      "    ...получасе ходьбы : NOUN -> NOUN\n",
      "    ......ходьбы от : NOUN -> PREP\n",
      "    .........от университета : PREP -> NOUN\n",
      "\n",
      "оглушительно захлопали по створкам окна:\n",
      "    оглушительно захлопали : ADVB <- VERB\n",
      "    захлопали по : VERB -> PREP\n",
      "    ...по створкам : PREP -> NOUN\n",
      "    ......створкам окна : NOUN -> NOUN\n",
      "\n",
      "потому вскоре пришел к выводу:\n",
      "    пришел к : VERB -> PREP\n",
      "    ...к выводу : PREP -> NOUN\n",
      "    вскоре пришел : ADVB <- VERB\n",
      "    ...потому вскоре : ADVB <- ADVB\n",
      "\n",
      "робко пытался подняться с пола:\n",
      "    робко пытался : ADVB <- VERB\n",
      "    пытался подняться : VERB -> INFN\n",
      "    ...подняться с : INFN -> PREP\n",
      "    ......с пола : PREP -> NOUN\n",
      "\n",
      "впредь стану заходить к нему:\n",
      "    впредь стану : ADVB <- VERB\n",
      "    стану заходить : VERB -> INFN\n",
      "    ...заходить к : INFN -> PREP\n",
      "    ......к нему : PREP -> NPRO\n",
      "\n",
      "после чего доковылял до двери:\n",
      "    доковылял до : VERB -> PREP\n",
      "    ...до двери : PREP -> NOUN\n",
      "    чего доковылял : ADVB <- VERB\n",
      "    ...после чего : ADVB <- ADVB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "forest_repr = parser.forest_repr()\n",
    "for main_w, tree_words in list(sorted(list(forest_repr.items()),\n",
    "                                      key = lambda tree: len(tree[1][0]),\n",
    "                                      reverse = True))[:10]:\n",
    "    print('{}:\\n{}\\n'.format(' '.join(tree_words[0]), \n",
    "                           '\\n'.join(map(lambda trio: '    {}{} : {}'.format(trio[0] * '...', *trio[1:]), \n",
    "                                         tree_words[1]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можем видеть, что словосочетания стали гораздо сложнее: вместе с тем, они не потеряли осмысленности. Отметим тот факт, что в большинстве выведенных нами случаев вершиной дерева является глагол, что соответствует идее грамматики зависимостей, где вершиной предложения признается глагольная форма.\n",
    "\n",
    "Исключения составляют те словосочетания, вершина которых находится на расстоянии больше, чем 1, от слова, от которого они зависимы: например, от глагола. Поэтому следующим естественным шагом является увеличение параметра distance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Увеличение параметра distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = Parser('text_1.txt', distance = 2)\n",
    "parser.text_parse()\n",
    "graph_repr = parser.graph_repr()\n",
    "forest_repr = parser.forest_repr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, как выглядят списки смежности в нашем графе (то есть слова, зависимые от одного):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "желание : свое давнее желание выглянуть из : ADJF <- NOUN, ADJF <- NOUN, NOUN -> INFN, NOUN -> PREP\n",
      "опустился : чего снова опустился на стул : ADVB <- VERB, ADVB <- VERB, VERB -> PREP, VERB -> NOUN\n",
      "постучал : несколько раз постучал в дверь : ADVB <- VERB, ADVB <- VERB, VERB -> PREP, VERB -> NOUN\n",
      "доковылял : после чего доковылял до двери : ADVB <- VERB, ADVB <- VERB, VERB -> PREP, VERB -> NOUN\n",
      "душу : мучившую его душу тайну : PRTF <- NOUN, ADJF <- NOUN, VERB -> NOUN\n",
      "желание : своенравное желание выглянуть из : ADJF <- NOUN, NOUN -> INFN, NOUN -> PREP\n",
      "свечу : еще одну свечу в : ADVB <- VERB, ADJF <- NOUN, VERB -> PREP\n",
      "превышавшей : намного превышавшей уровень крыш : ADVB <- PRTF, PRTF -> NOUN, PRTF -> NOUN\n",
      "привести : отчаянно привести старика в : ADVB <- INFN, INFN -> NOUN, INFN -> PREP\n",
      "находилось : все что находилось в : ADVB <- VERB, ADVB <- VERB, VERB -> PREP\n"
     ]
    }
   ],
   "source": [
    "for main_w, bond_words in list(sorted(list(graph_repr.items()),\n",
    "                                      key = lambda item: len(item[1][0]),\n",
    "                                      reverse = True))[:10]:\n",
    "    print('{} : {} : {}'.format(parser[main_w], ' '.join(bond_words[0]), ', '.join(bond_words[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При увеличении distance мы получаем не самый верный граф зависимостей, это касается сочетания существительных и предлогов: подразумевается, что их связь неразрывна (в отличие от того же согласования или управление глагола существительным). Мы не будем переписывать наш класс, поскольку исключения касаются не только правил N -> Prep и Prep -> N, и это как раз и является правилами для использования правил синтаксических связей, которые мы и не собирались вырабатывать."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Посмотрим теперь, как строятся деревья.\n",
    "\n",
    "С увеличением параметра distance возможны связи, которые неверны в контексте (при distance = 1 мы можем быть уверены в том, что эти слова были соседними в исходном тексте): поэтому будем выводить предложения, содержащие найденные деревья."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "По массивному, сложенному из  черного камня мосту  улица эта пересекала\n",
      "темную реку, вдоль которой располагались кирпичные стены складских помещений\n",
      "с  помутневшими  окнами\n",
      "\n",
      "вдоль располагались кирпичные стены складских помещений с помутневшими окнами:\n",
      "    располагались стены : VERB -> NOUN\n",
      "    ...кирпичные стены : ADJF <- NOUN\n",
      "    ...стены помещений : NOUN -> NOUN\n",
      "    ......помещений с : NOUN -> PREP\n",
      "    .........с окнами : PREP -> NOUN\n",
      "    ............помутневшими окнами : PRTF <- NOUN\n",
      "    ......складских помещений : ADJF <- NOUN\n",
      "    вдоль располагались : ADVB <- VERB\n",
      "\n",
      "Изредка\n",
      "попадались и такие места, где как бы падавшие друг другу навстречу дома чуть\n",
      "ли не  смыкались своими крышами, образуя некое  подобие арки\n",
      "\n",
      "где как падавшие друг другу навстречу дома ли:\n",
      "    падавшие друг : PRTF -> NOUN\n",
      "    как падавшие : ADVB <- PRTF\n",
      "    ...где как : ADVB <- ADVB\n",
      "    падавшие другу : PRTF -> NOUN\n",
      "    ...другу дома : NOUN -> NOUN\n",
      "    ......навстречу дома : ADVB <- ADVB\n",
      "    ......дома ли : NOUN -> NOUN\n",
      "\n",
      "Как-то раз днем, когда\n",
      "Занн был в театре, я даже хотел, было, подняться в мансарду,  однако дверь в\n",
      "нее оказалась заперта\n",
      "\n",
      "Как-то раз днем когда Занн был в театре:\n",
      "    был в : VERB -> PREP\n",
      "    ...в театре : PREP -> NOUN\n",
      "    когда был : ADVB <- VERB\n",
      "    ...днем когда : ADVB <- ADVB\n",
      "    ......днем Занн : NOUN -> NOUN\n",
      "    ......Как-то днем : ADVB <- ADVB\n",
      "    ......раз днем : ADVB <- ADVB\n",
      "    .........Как-то раз : ADVB <- ADVB\n",
      "    ...раз когда : ADVB <- ADVB\n",
      "\n",
      "Очевидно, мнение  Эриха Занна  о  комфортабельном  жилище  лежало\n",
      "далеко за пределами традиционных представлений на этот счет\n",
      "\n",
      "лежало за пределами традиционных представлений на этот счет:\n",
      "    лежало за : VERB -> PREP\n",
      "    ...за пределами : PREP -> NOUN\n",
      "    ......пределами представлений : NOUN -> NOUN\n",
      "    .........представлений на : NOUN -> PREP\n",
      "    ............на счет : PREP -> NOUN\n",
      "    ...............этот счет : ADJF <- NOUN\n",
      "    .........традиционных представлений : ADJF <- NOUN\n",
      "\n",
      "Если  не  считать\n",
      "недавнего подслушивания под дверями его квартиры, мне еще никогда в жизни не\n",
      "доводилось слышать ничего подобного\n",
      "\n",
      "считать недавнего подслушивания под дверями его квартиры:\n",
      "    считать подслушивания : INFN -> NOUN\n",
      "    ...недавнего подслушивания : ADJF <- NOUN\n",
      "    ...подслушивания под : NOUN -> PREP\n",
      "    ......под дверями : PREP -> NOUN\n",
      "    .........дверями квартиры : NOUN -> NOUN\n",
      "    ............его квартиры : ADJF <- NOUN\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for main_w, tree_words in list(sorted(list(forest_repr.items()),\n",
    "                                      key = lambda tree: len(tree[1][0]),\n",
    "                                      reverse = True))[:5]:\n",
    "    print('{}\\n\\n{}:\\n{}\\n'.format(tree_words[2], ' '.join(tree_words[0]), \n",
    "                           '\\n'.join(map(lambda trio: '    {}{} : {}'.format(trio[0] * '...', *trio[1:]), \n",
    "                                         tree_words[1]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далеко не все деревья представляют собой правильный разбор: сказывается то, что не любая пара слов, разделенная словом, может вообще анализиоваться нашими правилам. Тем не менее, первое словосочетание представляет собой пример очень хорошего разбора:\n",
    "\n",
    "<b><i>вдоль располагались кирпичные стены складских помещений с помутневшими окнами</i></b>\n",
    "\n",
    "данное словосочетание не только было разобрано правильно, но еще и практически полностью совпадает с частью исходного сложноподчиненного предложения (с исключенным из него словом \"которой\", связывающим эту часть с основной).\n",
    "\n",
    "На самом деле, связь \"располагались стены\" случайна, поскольку у нас нет правила, устанавливающего связь предиката и субъекта, а у слова \"стены\" совпадают формы именительного и винительного падежа; несмотря на это, разбор не потерял от этого своей правильности.\n",
    "\n",
    "Заметим также, что несмотря на то, что мы не пытались разбивать предложения на составные части по запятым, найденные словосочетания все равно находятся в одной из частей предложений; таким образом, построенные деревья для сложного предложения соответствуют его частям.\n",
    "\n",
    "Есть все основания полагать, что на более простых текстах (менее литературных) построенный нами парсер будет выделять правильные словосоччетания с их правильным разбором."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
